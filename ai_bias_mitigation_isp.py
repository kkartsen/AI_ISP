# -*- coding: utf-8 -*-
"""AI_Bias_Mitigation_ISP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SBVvw_QjsstotJUoRgrwwl3DsxCA1vOt

This project utilizes a dataset that is frequently encountered in ML spaces and is comprised of census data, and therefore contains multiple social categories from which bias can arise. The version used here is from kaggle, a popular dataset repository.

##**Adding the Kaggle Dataset to Colab**

**Step 1.** Install kaggle python library
"""

! pip install -q kaggle

"""**Step 2.** Upload kaggle API key"""

from google.colab import files
files.upload()

"""**Step 3.** Store kaggle API token in new kaggle folder"""

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/

"""**Step 4.** Change file permissions for API key to r&w for owner"""

! chmod 600 ~/.kaggle/kaggle.json

"""**Step 5.** Check connection to kaggle established and functioning"""

! kaggle datasets list

"""**Step 6.** Download dataset"""

! kaggle datasets download wenruliu/adult-income-dataset

"""**Step 7.** Unzip dataset"""

! mkdir ds
! unzip adult-income-dataset.zip -d ds

"""##**Analyzing the Dataset**"""

import pandas as pd

"""**Step 1.** Load the dataset into a pandas dataframe"""

df = pd.read_csv("ds/adult.csv")

"""**Step 2.** Observe column headers and first instances"""

df.head()

"""**Step 3.** Get number of rows and columns"""

df.shape

"""**Step 4.** Get statistical information of dataset"""

df.describe()

"""**Step 5.** Get summary of dataset"""

df.info()

"""There are 48,842 rows of data and each column contains 48,842 non-null values. This means there are no NaN values to account for. However, we can see in row 4 that some values are shown as '?', so not all values for all instances were collected. This should be taken into consideration to ensure it does not skew results, either because too many values in a column are '?', or because '?' values are unevenly distributed.

##**Cleaning the Dataset**
"""

import matplotlib.pyplot as plt # for data visualization

"""**Step 1.** Address rows with '?' values

**1a.** Find columns that have '?' values

**Note: only columns with '?' values are shown in this step*
"""

df['workclass'].unique()

df['occupation'].unique()

df['native-country'].unique()

"""**1b.** Check how many values in each of these four columns have '?' values

**Note: if many rows have a '?' in a particular column, it is likely a poor decision to purge them from the dataset, since this could result in too few instances to find reliable relationships between the data*
"""

(df['workclass'] == '?').sum()

(df['occupation'] == '?').sum()

(df['native-country'] == '?').sum()

"""While there are hundreds or thousands of rows with '?' values for each of these columns, they each make up a small portion of the overall dataset. Therefore, this information alone is not enough to determine with certainty that the rows with '?' values for these columns should be kept. For this reason, we move on to step 3c for each of these columns.

**1c.** Check if removing '?' values skews the data

**Note: if this is the case, purging rows containing '?' values could result in skewed data*
"""

#Step 3c(i). Filter the dataset to remove entries with a '?' in the workclass column
filter_workclass = df[df['workclass'] != '?']

fig, axes = plt.subplots(1, 2, figsize=(10, 4))
axes[0].hist(filter_workclass['occupation'], bins=15)
axes[0].tick_params(axis='x', rotation=90) # Rotate x-axis labels by 90 degrees
axes[1].hist(df['occupation'], bins=15)
axes[1].tick_params(axis='x', rotation=90) # Rotate x-axis labels by 90 degrees
plt.tight_layout()

"""While all other independent variables retain the shape of their distributions (not shown) when entries with unknown workclass are removed, plotting the occupation column as in the histograms above reveals that approximately all entries with unknown workclass also have unknown occupation. Note that the graph on the right, which uses the original dataset, shows almost 3000 '?' entries, while the graph on the left, which has entries with missing workclass removed, shows none. This means that these rows may not contain enough data to be valuable in training, and should be removed if the values cannot be filled in somewhat reliably through imputation."""

#Step 3c(i). Filter the dataset to remove entries with a '?' in the occupation column
filter_occupation = df[df['occupation'] != '?']

fig, axes = plt.subplots(1, 2, figsize=(10, 4))
axes[0].hist(filter_occupation['workclass'], bins=15)
axes[0].tick_params(axis='x', rotation=90) # Rotate x-axis labels by 90 degrees
axes[1].hist(df['workclass'], bins=15)
axes[1].tick_params(axis='x', rotation=90) # Rotate x-axis labels by 90 degrees
plt.tight_layout()

"""Checking against the occupation column reveals the same information as checking against the workclass column did. This makes sense since the entries removed are generally the same in both cases."""

#Step 3c(i). Filter the dataset to remove entries with a '?' in the workclass column
filter_nativecountry = df[df['native-country'] != '?']

fig, axes = plt.subplots(1, 2, figsize=(10, 4))
axes[0].hist(filter_nativecountry['workclass'], bins=15)
axes[0].tick_params(axis='x', rotation=90) # Rotate x-axis labels by 90 degrees
axes[1].hist(df['workclass'], bins=15)
axes[1].tick_params(axis='x', rotation=90) # Rotate x-axis labels by 90 degrees
plt.tight_layout()

"""From this histogram, we can see that the rows with missing values for native-country did not significantly overlap with rows with missing values for workclass. Neither did these rows overlap with rows with missing values for occupation, nor did removing these rows affect the distribution of data.

Based on these results, and the impractibility of data imputation in this particular case, we can determine that the best course of action is to remove the entries with '?' values.

**1d.** Clean the data of rows that contain '?' (missing) values
"""

df = df[df['workclass'] != '?']
df = df[df['occupation'] != '?']
df = df[df['native-country'] != '?']

"""**Step 2.** Clean the data of columns with bad or not useful data

Analysis of the capital-gain and capital-loss columns shows that 92% and 95% of their values are 0, respectively. Therefore, these columns are not useful and should be purged from the dataframe
"""

(df['capital-gain'] == 0).sum() / df.shape[0]

(df['capital-loss'] == 0).sum() / df.shape[0]

fig, axes = plt.subplots(1, 2, figsize=(10, 4))
#ax.tick_params(axis='x', rotation=45) # Rotate x-axis labels by 45 degrees
axes[0].hist(df['capital-gain'], bins=123)
axes[1].hist(df['capital-loss'], bins=74)
plt.tight_layout()

df.drop(['capital-gain', 'capital-loss'], axis=1, inplace=True)

"""The fnlwgt column is the number of people within the population a datapoint is believed to represent. While this value is important for population analysis, it is not directly helpful for making predictions about individual income, which is what the ML model will do. Therefore, this column is also removed."""

df.drop(['fnlwgt'], axis=1, inplace=True)

"""##**Preprocessing the Data**

Before the model can be run on the dataset and metrics used for analysis, the data must be formatted in a way to make it compatible
"""

from sklearn.preprocessing import OneHotEncoder

"""**Step 1.** Change data label values from object type to boolean type

The job of the model is to predict the value of the income column for each instance. Currently, the possible income values are '<=50K' and '>50K'. In order to use the fairness metrics effectively, the predictions should be positive or negative. To do that, this column will be changed to boolean values where income is positive if >50K or negative if <=50K.
"""

df['income'] = df['income'] == '>50K'

df.head()

"""**Step 2.** Change ordinal categorical data types to numerical data types

The educational-num column represents the same data in the the education column, but in numeric form. This is possible because level of education is ordinal data, from least to most. Since both columns contain the same information and the model requires numerical data in order to be run on a traditional computer, the education column is dropped.
"""

X = df['educational-num']
Y = df['education']

plt.scatter(X, Y)

df.drop(['education'], axis=1, inplace=True)

"""**Step 3.** Change nominal categorical data types to numerical data types"""

#Use OneHotEncoder for conversion without introducing bias or ordering nominal data
ohe = OneHotEncoder(sparse_output=False).set_output(transform='pandas') #set output to dataframe instead of sparse matrix

#transform categorical data to numerical data for each relevant column
ohe_workclass = ohe.fit_transform(df[['workclass']])
ohe_marital = ohe.fit_transform(df[['marital-status']])
ohe_occupation = ohe.fit_transform(df[['occupation']])
ohe_relationship = ohe.fit_transform(df[['relationship']])
ohe_race = ohe.fit_transform(df[['race']])
ohe_gender = ohe.fit_transform(df[['gender']])
ohe_native = ohe.fit_transform(df[['native-country']])

#add one-hot columns to dataframe
df = pd.concat([df, ohe_workclass, ohe_marital, ohe_occupation, ohe_relationship, ohe_race, ohe_gender, ohe_native], axis=1)

#drop categorical columns from dataframe
df.drop(['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country'], axis=1, inplace=True)

df.info()

"""##**Building the ML Model & Running It on the Dataset**

This project will use a decision tree ML model. This algorithm has many benefits. It is relatively simple to program and analyze, it has multiple hyperparameters that can be adjusted when testing how changes to the model affect bias in the output, and it is useful for categorical prediction (in this case, income greater than \$50k or income less than \$50k). Decision tree hyperparameters include maximum depth of the tree, minimum samples required to split a node, minimum samples required at a leaf, and maximum number of features to consider when splitting a node.
"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

#This creates a new dataframe for this section, so df can still be used later if wanted without changes made here
unaltered_df = df.copy()

"""**Step 1.** Split dataframe into feature matrix and labels"""

X = unaltered_df.drop(['income'], axis=1, inplace=False)
y = unaltered_df['income']

"""**Step 2.** Split data into training and testing data"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  #20% of data reserved for testing

"""**Step 3.** Train the model

For this project, since the focus is not on creating the model itself, the DecisionTreeClassifier functions from sklearn are used.

Additionally, this Decision Tree model uses the gini attribute for splitting instances. It has some slight advantages over entropy, but overall the decision to use one vs another is not very significant.

"""

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

"""**Step 4.** Test the model"""

predictions = model.predict(X_test)

"""##**Evaluating the Model on the Original Dataset (Bias Unchanged)**

Different use cases call for different metrics by which to measure bias. For example, an ML algorithm used by a bank for the purpose of deciding whether to approve a loan or not should measure **demographic parity** to ensure an **equal probability of positive outcomes across different demographic groups**.

The original purpose of the dataset used in this project was to make accurate predictions about yearly income. Due to social, regulatory, or other factors, it may not always be the case that individuals with different demographics are equally likely to be part of a certain group. For example, a model that predicts the major of a college student might be more accurate if predicts that male students are mechanical engineering majors at a higher rate than it does for female students. In cases such as this, using **equalized odds** would ensure an **equal probability that positive predictions are correct and incorrect across different demographic groups**.

In this project, bias shall be measured using both of these metrics to take various use cases into consideration.

1. Predictive Parity
2. Equalized Odds
"""

!pip install fairlearn

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
from fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio, MetricFrame, selection_rate, equalized_odds_difference

"""###**Part 1.** Checking model accuracy"""

accuracy = accuracy_score(y_test, predictions)
print(f"{accuracy:.2%}")

"""###**Part 2.** Checking model fairness: demographic parity

**Step 1.** Calculate Demographic Parity for Gender
"""

mf_gender_dp = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test,   #for demographic parity, true values don't matter
    y_pred=predictions,
    sensitive_features=X_test[['gender_Male', 'gender_Female']]
)

"""Selection rate is the probability of a positive prediction for a certain demographic. Here, the selection rate for gender_Male is close to 0.30, and the selection rate for gender_Female is close to 0.12. This means the model, trained on the given training data, is 2.5x more likely to predict an income greater than $50k for men than women. The NaN values show there are no instances where gender_Male and gender_Female have the same value (0.0 or 1.0). This makes sense, because these features are one-hot encoded from a single column."""

#likelihood of positive prediction by gender
display(mf_gender_dp.by_group)

"""The overall selection rate is about 0.24, which is the average of the selection rates for each group. This will be the same for any MetricFrame with the same data (regardless of chosen sensitive features).

"""

mf_gender_dp.overall

"""Demographic parity difference measures the absolute difference between the highest and lowest selection rates across groups. Values range from 0.0 to 1.0, with 0.0 signifying demographic parity (equal selection rates). The fairness range, or values for which the results are considered fair, for this metric is not concrete, but may be between 0.0 and 0.25. Here, demographic parity difference is, on average, 0.18, which is on the higher end of that fairness range.

"""

dp_g_ob_1 = demographic_parity_difference(
    y_true=y_test,
    y_pred=predictions,
    sensitive_features=X_test[['gender_Male', 'gender_Female']]
)
print(dp_g_ob_1)

"""**Step 2.** Calculate Demographic Parity for Race"""

mf_race_dp = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test,
    y_pred=predictions,
    sensitive_features=X_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the approximate selection rate for race_Asian-Pac-Islander is 0.27, for race_White is 0.25, for race_Other is 0.18, for race_Black is 0.16, and for race_Amer-Indian-Eskimo is 0.12. There are more NaN values in this table, because there are more possible values in the 'race' column."""

#likelihood of positive prediction by gender
display(mf_race_dp.by_group)

"""Here, demographic parity difference is, on average, 0.15, which is within the fairness range."""

dp_r_ob_1 = demographic_parity_difference(
    y_true=y_test,
    y_pred=predictions,
    sensitive_features=X_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)
print(dp_r_ob_1)

"""###**Part 3.** Checking model fairness: equalized odds

**Step 1.** Calculate Equalized Odds for Gender

True positive rate (TPR) is the probability a positively labeled instance is predicted positive: ðšƒð™¿ðš = ðšƒð™¿ / (ðšƒð™¿ + ð™µð™½)
"""

mf_gender_tpr = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test,
    y_pred=predictions,
    sensitive_features=X_test[['gender_Male', 'gender_Female']]
)

"""Here, the TPR for gender_Male is about 0.53, and the TPR for gender_Female is about 0.47. This means the model, trained on the given training data, is slightly more likely to accurately predict an income greater than $50k for men than women. Like when displaying statistical parity, the NaN values show there are no instances where the different features have the same value, since they are one-hot encoded from a single column."""

display(mf_gender_tpr.by_group)

"""False positive rate (FPR) is the probability a negatively labeled instance is predicted positive:
ð™µð™¿ðš = ð™µð™¿ / (ð™µð™¿ + ðšƒð™½)
"""

# FPR: need a small helper that computes FPR
def fpr(y_t, y_p):
    # ensure binary 0/1 labels
    tn, fp, fn, tp = confusion_matrix(y_t, y_p).ravel()
    return fp / (fp + tn)

mf_gender_fpr = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test,
    y_pred=predictions,
    sensitive_features=X_test[['gender_Male', 'gender_Female']]
)

"""Here, the FPR for gender_Male is about 0.19, and the FPR for gender_Female is about 0.06. This means the model, trained on the given training data, is more likely to inaccurately predict an income greater than $50k for men than women."""

display(mf_gender_fpr.by_group)

"""The TPR gap is 0.15, which is the absolute difference between the highest and lowest TPRs across groups. The FPR gap is 0.12, which is the absolute difference between the highest and lowest FPRs across groups. Values range from 0.0 to 1.0, with 0.0 signifying less bias (equal TPRs/FPRs)"""

tpr_gap = mf_gender_tpr.difference('between_groups')
fpr_gap = mf_gender_fpr.difference('between_groups')
print("TPR gap (max-min):", tpr_gap)
print("FPR gap (max-min):", fpr_gap)

"""Equalized Odds takes the greater of TPR or FPR. Values range from 0.0 to 1.0, with 0.0 signifying the odds (chances of correct/incorrect positive prediction) are the same. A low value means both TPR and FPR must be low, where a high value means at least one value is high, and provides no info about the other. The fairness range, or values for which the results are considered fair, for this metric is not concrete, but may be between 0.0 and 0.25. Here, the result for equalized odds is, on average, 0.12, which is right in the middle of that fairness range."""

eo_g_ob_1 = equalized_odds_difference(
    y_true=y_test,
    y_pred=predictions,
    sensitive_features=X_test[['gender_Male', 'gender_Female']],
)

print(eo_g_ob_1)

"""**Step 2.** Calculate Equalized Odds for Race"""

mf_race_tpr = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test,
    y_pred=predictions,
    sensitive_features=X_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

mf_race_fpr = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test,
    y_pred=predictions,
    sensitive_features=X_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the TPR for race_White is 0.53, for race_Asian-Pac-Islander is 0.49, for race_Black is 0.45, for race_Amer-Indian-Eskimo is 0.50, and for race_Other is 0.50. From this, we can see the model about as likely to accurately predict an income greater than $50k for all individuals, regardless of race."""

display(mf_race_tpr.by_group)

"""Here, the FPR for race_White is 0.14, for race_Asian-Pac-Islander is 0.20, for race_Black is 0.09, for race_Amer-Indian-Eskimo is 0.13, and for race_Other is also 0.13. From this, we can see the model is about as likely to inaccurately predict an income greater than $50k for individuals who identify as White, American Indian/Eskimo, or Other. It is more likely to do so for those who identify as Asian/Pacific Islander, and less likely to do so for those who identify as Black."""

display(mf_race_fpr.by_group)

"""The TPR gap is 0.47, and the FPR gap is 0.10. These are both quite low values."""

tpr_gap = mf_race_tpr.difference('between_groups')
fpr_gap = mf_race_fpr.difference('between_groups')
print("TPR gap (max-min):", tpr_gap)
print("FPR gap (max-min):", fpr_gap)

"""Here, the result for equalized odds is, on average, 0.10, which is comfortably within the fairness range."""

eo_r_ob_1 = equalized_odds_difference(
    y_true=y_test,
    y_pred=predictions,
    sensitive_features=X_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_ob_1)

"""##**Altering the Dataset**"""

import numpy as np

"""###**Part 1.** Increasing bias in the dataset"""

#This creates a new version of the dataframe for this section
more_bias_df = df.copy()

"""**Step 1.** Increase gender bias

**1a.** Reduce positive labels for female instances

This will be done by flipping half the instances (selected randomly) where gender = Female and income > \$50k to income <= $50k

This should reduce the likelihood of the model to predict a positive outcome for instances where gender is female, and/or reduce the likelihood that a positive outcome for instances where gender is female is a true positive.
"""

#Get instances where gender is female and income is >$50k
fem_pos = more_bias_df[(more_bias_df['gender_Female'] == 1) & (more_bias_df['income'] == True)]

#Randomly relable half of fem_pos to be fem_neg
n_flip_fem = int(fem_pos.shape[0] * 0.50)  #flip 50% of positive instances
flip_indices_fem = fem_pos.sample(n=n_flip_fem).index
more_bias_df.loc[flip_indices_fem, 'income'] = False

"""**1b.** Increase positive labels for male instances

This will be done by flipping half the instances (selected randomly) where gender = Male and income <= \$50k to income > $50k

This should increase the likelihood of the model to predict a positive outcome for instances where gender is male, and/or increase the likelihood that a positive outcome for instances where gender is male is a true positive.
"""

#Get instances where gender is male and income is <=$50k
mal_neg = more_bias_df[(more_bias_df['gender_Male'] == 1) & (more_bias_df['income'] == False)]

#Randomly relable half of mal_neg to be mal_pos
n_flip_mal = int(mal_neg.shape[0] * 0.50)  #flip 50% of negative instances
flip_indices_mal = mal_neg.sample(n=n_flip_mal).index
more_bias_df.loc[flip_indices_mal, 'income'] = True

"""**Step 2.** Increase racial bias

The races with both the highest selection rates and precision scores are White and Asian-Pac-Islander. We will increase the bias so that instances with these two values for race have an even higher likelihood of positive and/or true positive predictions when compared to instances where race is Black, Amer-Indian-Eskimo, or Other.

**2a.** Reduce positive labels for Black, Amer-Indian-Eskimo, and Other races
"""

#Get instances where race is Black, Amer-Indian-Eskimo, or Other, and income is >$50k
black_pos = more_bias_df[(more_bias_df['race_Black'] == 1) & (more_bias_df['income'] == True)]
AIE_pos = more_bias_df[(more_bias_df['race_Amer-Indian-Eskimo'] == 1) & (more_bias_df['income'] == True)]
other_pos = more_bias_df[(more_bias_df['race_Other'] == 1) & (more_bias_df['income'] == True)]

#Randomly relable half of each of these instances to be negative

#race_Black
n_flip_black = int(black_pos.shape[0] * 0.50)  #flip 50% of positive instances
flip_indices_black = black_pos.sample(n=n_flip_black).index
more_bias_df.loc[flip_indices_black, 'income'] = False

#race_Amer-Indian-Eskimo
n_flip_AIE = int(AIE_pos.shape[0] * 0.50)
flip_indices_AIE = AIE_pos.sample(n=n_flip_AIE).index
more_bias_df.loc[flip_indices_AIE, 'income'] = False

#race_Other
n_flip_other = int(other_pos.shape[0] * 0.50)
flip_indices_other = other_pos.sample(n=n_flip_other).index
more_bias_df.loc[flip_indices_other, 'income'] = False

"""**2b.** Increase positive labels for White and Asian-Pac-Islander races"""

#Get instances where race is White or Asian-Pac-Islander, and income is <=$50k
white_neg = more_bias_df[(more_bias_df['race_White'] == 1) & (more_bias_df['income'] == False)]
API_neg = more_bias_df[(more_bias_df['race_Asian-Pac-Islander'] == 1) & (more_bias_df['income'] == False)]

#Randomly relable half of each of these instances to be positive

#race_White
n_flip_white = int(white_neg.shape[0] * 0.50)  #flip 50% of negative instances
flip_indices_white = white_neg.sample(n=n_flip_white).index
more_bias_df.loc[flip_indices_white, 'income'] = True

#race_Asian-Pac-Islander
n_flip_API = int(API_neg.shape[0] * 0.50)
flip_indices_API = API_neg.sample(n=n_flip_API).index
more_bias_df.loc[flip_indices_API, 'income'] = True

"""###**Part 2.** Decreasing bias in the dataset"""

#This creates a new version of the dataframe for this section
less_bias_df = df.copy()

"""**Step 1.** Decrease gender bias

**1a.** Get overall positive rate
"""

pos_rate = (less_bias_df['income'] == True).mean()

"""**1b.** Identify rows to resample"""

male_pos = less_bias_df[(less_bias_df['gender_Male'] == 1) & (less_bias_df['income'] == True)]
male_neg = less_bias_df[(less_bias_df['gender_Male'] == 1) & (less_bias_df['income'] == False)]

female_pos = less_bias_df[(less_bias_df['gender_Female'] == 1) & (less_bias_df['income'] == True)]
female_neg = less_bias_df[(less_bias_df['gender_Female'] == 1) & (less_bias_df['income'] == False)]

"""**1c.** Resample rows to match overall positive rate"""

#gender_Male
n_male = len(less_bias_df[less_bias_df['gender_Male'] == 1])
n_male_pos = int(n_male * pos_rate)
resample_male_pos = less_bias_df[(less_bias_df['gender_Male'] == 1) & (less_bias_df['income'] == True)].sample(n=n_male_pos, replace=True)
resample_male_neg = less_bias_df[(less_bias_df['gender_Male'] == 1) & (less_bias_df['income'] == False)].sample(n=n_male - n_male_pos, replace=True)

#gender_Female
n_female = len(less_bias_df[less_bias_df['gender_Female'] == 1])
n_female_pos = int(n_female * pos_rate)
resample_female_pos = less_bias_df[(less_bias_df['gender_Female'] == 1) & (less_bias_df['income'] == True)].sample(n=n_female_pos, replace=True)
resample_female_neg = less_bias_df[(less_bias_df['gender_Female'] == 1) & (less_bias_df['income'] == False)].sample(n=n_female - n_female_pos, replace=True)

"""**1d.** Drop original rows and add back resampled rows"""

indices_to_drop = male_pos.index.union(male_neg.index).union(female_pos.index).union(female_neg.index)
less_bias_df = less_bias_df.drop(indices_to_drop)


less_bias_df = pd.concat([less_bias_df, resample_male_pos, resample_male_neg])
less_bias_df = pd.concat([less_bias_df, resample_female_pos, resample_female_neg])

"""**Step 2.** Decrease racial bias

**2a.** Get overall positive rate-- already done in Step 1a

**2b.** Resample each race to match overall positive rate
"""

races = ['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']

for race in races:
  #identify rows to resample
  race_pos = less_bias_df[(less_bias_df[race] == 1) & (less_bias_df['income'] == True)]
  race_neg = less_bias_df[(less_bias_df[race] == 1) & (less_bias_df['income'] == False)]

  #resample rows to match overall positive rate
  n_race = len(less_bias_df[less_bias_df[race] == 1])
  pos = int(n_race * pos_rate)
  resample_pos = less_bias_df[(less_bias_df[race] == 1) & (less_bias_df['income'] == True)].sample(n=pos, replace=True)
  resample_neg = less_bias_df[(less_bias_df[race] == 1) & (less_bias_df['income'] == False)].sample(n=n_race - pos, replace=True)

  #drop original rows
  indices_to_drop = race_pos.index.union(race_neg.index)
  less_bias_df = less_bias_df.drop(indices_to_drop)

  #add back resampled rows
  less_bias_df = pd.concat([less_bias_df, resample_pos, resample_neg])

"""**Step 3.** Shuffle the data to prevent the decision tree from making inferences based on the organization of the data during training-- currently it is grouped by race, with each race grouped by gender, and each group of gender within a race grouped by income"""

less_bias_df = less_bias_df.sample(frac=1).reset_index(drop=True)

"""##**Analyzing the Altered Datasets**

###**Part 1.** Analyzing the more biased sataset

**Step 1.** Observe column headers and first instances
"""

more_bias_df.head()

"""**Step 2.** Get number of rows and columns

There should be fewer rows due to data cleaning eliminating rows with unknown values, and more columns due to data preprocessing expanding nominal categorical data into one-hot encoded columns
"""

more_bias_df.shape

"""**Step 3.** Get statistical information of dataset

Columns of one-hot encoded data with a value of 1 for lower quartiles indicate that column is a popular value among the unique values of the original categorical data column.
"""

more_bias_df.describe()

"""**Step 4.** Get summary of dataset

Here we can verify that all categorical data has been transformed into numerical data, and we have columns representing each unique value from the original categorical data columns
"""

more_bias_df.info()

"""###**Part 2.** Analyzing the less biased dataset

**Step 1.** Observe column headers and first instances
"""

less_bias_df.head()

"""**Step 2.** Get number of rows and columns"""

less_bias_df.shape

"""**Step 3.** Get statistical information of dataset"""

less_bias_df.describe()

"""**Step 4.** Get summary of dataset"""

less_bias_df.info()

"""##**Running the Model on the Altered Datasets**

###**Part 1.** Running the model on the more biased dataset

**Step 1.** Split dataframe into feature matrix and labels
"""

Xmb = more_bias_df.drop(['income'], axis=1, inplace=False)
ymb = more_bias_df['income']

"""**Step 2.** Split data into training and testing data"""

Xmb_train, Xmb_test, ymb_train, ymb_test = train_test_split(Xmb, ymb, test_size=0.2)

"""**Step 3.** Train the model

"""

#model = DecisionTreeClassifier()  #uncomment to run again, already done in earlier section
model.fit(Xmb_train, ymb_train)

"""**Step 4.** Test the model"""

predictions_mb = model.predict(Xmb_test)

"""###**Part 2.** Running the model on the less biased dataset

**Step 1.** Split dataframe into feature matrix and labels
"""

Xlb = less_bias_df.drop(['income'], axis=1, inplace=False)
ylb = less_bias_df['income']

"""**Step 2.** Split data into training and testing data"""

Xlb_train, Xlb_test, ylb_train, ylb_test = train_test_split(Xlb, ylb, test_size=0.2)

"""**Step 3.** Train the model

"""

#model = DecisionTreeClassifier()  #uncomment to run again, already done in earlier section
model.fit(Xlb_train, ylb_train)

"""**Step 4.** Test the model"""

predictions_lb = model.predict(Xlb_test)

"""##**Evaluating the Model on the More Biased Dataset**

###**Part 1.** Evaluating more biased model for accuracy
"""

accuracy_mb = accuracy_score(ymb_test, predictions_mb)
print(f"{accuracy_mb:.2%}")

"""###**Part 2.** Evaluating more biased model for demographic parity

**Step 1.** Checking model fairness: Demographic Parity for Gender
"""

#Create MetricFrame
mf_gender_dp_mb = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=ymb_test,
    y_pred=predictions_mb,
    sensitive_features=Xmb_test[['gender_Male', 'gender_Female']]
)

"""Here, the selection rate for gender_Male is 0.77, and the selection rate for gender_Female is 0.42. For both, this is a significant positive change. This makes sense for gender_Male, because we did greatly increased the ratio of potive to negative labels. However for gender_Female, this may be indicative of the particular instances that were randomly chosen to be flipped."""

#likelihood of positive prediction by gender
display(mf_gender_dp_mb.by_group)

"""The overall selection rate is 0.65, which is much higher than the overall selection rate for the unaltered data. This is as expected, since the selection rate for all data was increased."""

mf_gender_dp_mb.overall

"""Here, demographic parity difference is, on average, 0.25, which is on the high side of the fairness range but still within it. It is noticeably higher than the demographic parity for gender in the unaltered data, as expected from increasing bias."""

dp_g_mb_1 = demographic_parity_difference(
    y_true=ymb_test,
    y_pred=predictions_mb,
    sensitive_features=Xmb_test[['gender_Male', 'gender_Female']]
)

print(dp_g_mb_1)

"""**Step 2.** Checking model fairness: Demographic Parity for Race"""

#Create MetricFrame
mf_race_dp_mb = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=ymb_test,
    y_pred=predictions_mb,
    sensitive_features=Xmb_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the selection rate for race_Asian-Pac-Islander is 0.66, for race_White is 0.71, for race_Amer-Indian-Eskimo is 0.11, for race_Black is 0.18, and for race_Other is 0.15. For race_Asian-Pac-Islander and race_White, this is a large increase from the unaltered data set, but a small change for race_Amer-Indian-Eskimo, race_Black, and race_Other. Because of our changes, the model, trained on the given training data, is much more likely to predict an income greater than $50k for individuals who identify as White or Asian than it is for any other race."""

#likelihood of positive prediction by race
display(mf_race_dp_mb.by_group)

"""Here, demographic parity difference is, on average, 0.59. This is much higher than the demographic parity for race in the unaltered data, as expected from increasing bias. This result would be very concerning in for a model representing a real-world situation where race does not correlate with the outcome. That would indicate a significant problem with the data."""

dp_r_mb_1 = demographic_parity_difference(
    y_true=ymb_test,
    y_pred=predictions_mb,
    sensitive_features=Xmb_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_mb_1)

"""###**Part 3.** Evaluating more biased model for equalized odds

**Step 1.** Checking model fairness: Equalized Odds for Gender
"""

mf_gender_tpr_mb = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=ymb_test,
    y_pred=predictions_mb,
    sensitive_features=Xmb_test[['gender_Male', 'gender_Female']]
)

mf_gender_fpr_mb = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=ymb_test,
    y_pred=predictions_mb,
    sensitive_features=Xmb_test[['gender_Male', 'gender_Female']]
)

"""Here, the TPR for gender_Male is 0.81, and for gender_Female is 0.51. From this, we can see the model is much more likely to accurately predict an income greater than $50k for individuals who identify as male than those who identify female. The TPR for gender_Male increased significantly from the original dataset, which makes sense, since we increased the number of positive male instances by half. This change means that even if the model is not very good at making predictions in general, on this data it can be right most of the time just by returning True for all instances where gender is male."""

display(mf_gender_tpr_mb.by_group)

"""Here, the FPR for gender_Male is about 0.64, and the FPR for gender_Female is about 0.36. This means the model, trained on the given training data, is more likely to inaccurately predict an income greater than $50k for men than women. Both measurements show a significant increase from the original dataset. This makes sense, because the data was altered somewhat randomly. Negative male values were turned positive, and positive female values were turned negative, interrupting the logic that the decision tree relies on to minimize false predictions."""

display(mf_gender_fpr_mb.by_group)

"""The TPR gap is 0.30, which is a lot higher than the TPR gap for the original dataset. This is expected, since the TPR for gender_Male data was significantly increased, while the TPR for gender_Female data did not change much."""

mf_gender_tpr_mb.difference('between_groups')

"""The FPR gap is 0.28, much higher than the FPR gap for the original dataset. This is expected because while FPR for both male and female data increased, the FPR for male data increased more, likely because more male data was chnged than female data."""

mf_gender_fpr_mb.difference('between_groups')

"""Here, the result for equalized odds is, on average, 0.30, which is just a bit outside of the fairness range (up to 0.25). This is a significant increase from the original data, which makes sense, as the data was deliberately altered to produce a "less fair" result by this metric."""

eo_g_mb_1 = equalized_odds_difference(
    y_true=ymb_test,
    y_pred=predictions_mb,
    sensitive_features=Xmb_test[['gender_Male', 'gender_Female']],
)

print(eo_g_mb_1)

"""**Step 2.** Checking model fairness: Equalized Odds for Race"""

mf_race_tpr_mb = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=ymb_test,
    y_pred=predictions_mb,
    sensitive_features=Xmb_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

mf_race_fpr_mb = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=ymb_test,
    y_pred=predictions_mb,
    sensitive_features=Xmb_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the TPR for race_White is 0.75, for race_Asian-Pac-Islander is 0.72, for race_Black is 0.27, for race_Amer-Indian-Eskimo is 0.29, and for race_Other is 0.29. From this, we can see the model is much more likely to accurately predict an income greater than $50k for individuals identifying as White or Asian/Pacific Islander than it is for those identifying as Black, American Indian/Eskimo, or another race. These changes show an increase or decrease of almost exactly half the original. This makes sense, since half the negative data for the races that showed an increase was flipped to positive, and half the positive data for the races that showed a decrease was flipped to negative."""

display(mf_race_tpr_mb.by_group)

"""Here, the FPR for race_White is 0.63, for race_Asian-Pac-Islander is 0.69, for race_Black is 0.16, for race_Amer-Indian-Eskimo is 0.10, and for race_Other is 0.18. From this, we can see the model is about as likely to inaccurately predict an income greater than $50k for individuals who identify as Black or Other. It is more likely to do so for those who identify as White or Asian/Pacific Islander, and less likely to do so for those who identify as American Indian/Eskimo. It makes sense that most of these values increased from the original dataset, and that White and Asian-Pac-Islander increased the most, since these two values had half their negative data taken away."""

display(mf_race_fpr_mb.by_group)

"""The TPR gap is 0.75, which is a lot higher than (about 10x) the TPR gap for the original dataset. This is expected, since the TPR for some data was significantly increased, while the TPR for other data was significatly decreased."""

mf_race_tpr_mb.difference('between_groups')

"""The FPR gap is 0.58, much higher than the FPR gap for the original dataset. This is expected because while FPR for both most data increased, and White and Asian-Pac-Islander by a lot, the FPR for other data only increased slightly or even slightly decreased, in the case of Amer-Indian-Eskimo."""

mf_race_fpr_mb.difference('between_groups')

"""Here, the result for equalized odds is, on average, 0.75, which is well outside the fairness range and indicates significant inequity. This result would be extremely concerning for a use case for which the model should perform equally well for all demographics. This result makes sense, since we increased bias."""

eo_r_mb_1 = equalized_odds_difference(
    y_true=ymb_test,
    y_pred=predictions_mb,
    sensitive_features=Xmb_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_mb_1)

"""##**Evaluating the Model on the Less Biased Dataset**

###**Part 1.** Evaluating less biased model for accuracy
"""

accuracy_lb = accuracy_score(ylb_test, predictions_lb)
print(f"{accuracy_lb:.2%}")

"""###**Part 2.** Evaluating less biased model for demographic parity

**Step 1.** Checking model fairness: Demographic Parity for Gender
"""

#Create MetricFrame
mf_gender_dp_lb = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=ylb_test,
    y_pred=predictions_lb,
    sensitive_features=Xlb_test[['gender_Male', 'gender_Female']]
)

"""Here, the selection rate for gender_Male is 0.24, and the selection rate for gender_Female is 0.26. For gender_Male, this is a very small change from the unaltered data set, but a significant positive change for gender_Female. Now, the selection rate for gender_Male data and gender_Female data is about equal. This makes sense, because we refactored the data for instances of both gender values to have the same rate of positive income values."""

#likelihood of positive prediction by gender
display(mf_gender_dp_lb.by_group)

"""The overall selection rate is 0.24, which is as expected, since the data was refactored specifically so that the positive rates for each value for gender would be the same as overall positive rate. Since there are many more instances for male data than female data, the overall selection rate for the less biased data should be between the values for the more biased data and closer to the value for male data. Here, we can see that it is."""

mf_gender_dp_lb.overall

"""Here, demographic parity difference is 0.03, which is very low, and significantly lower than the demographic parity for gender in the unaltered data, as expected from decreasing bias so that selection rates are approximately equal."""

dp_g_lb_1 = demographic_parity_difference(
    y_true=ylb_test,
    y_pred=predictions_lb,
    sensitive_features=Xlb_test[['gender_Male', 'gender_Female']]
)

print(dp_g_lb_1)

"""**Step 2.** Checking model fairness: Demographic Parity for Race"""

#Create MetricFrame
mf_race_dp_lb = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=ylb_test,
    y_pred=predictions_lb,
    sensitive_features=Xlb_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the selection rate for race_Asian-Pac-Islander is 0.26, for race_White is 0.24, for race_Amer-Indian-Eskimo is 0.30, for race_Black is 0.26, and for race_Other is 0.20. For race_Asian-Pac-Islander and race_White, this is a very small change from the unaltered data set, but a significant positive change for race_Amer-Indian-Eskimo, race_Black, and race_Other. This makes sense, because we altered income values for each data with race value to make the percentage of positive income values approximately the same. The result is that the model, trained on the given training data, is about as likely to predict an income greater than $50k for individuals of any race."""

#likelihood of positive prediction by race
display(mf_race_dp_lb.by_group)

"""Here, demographic parity difference is 0.09, which is very low and significantly lower than the demographic parity for race in the unaltered data, as expected from decreasing bias."""

dp_r_lb_1 = demographic_parity_difference(
    y_true=ylb_test,
    y_pred=predictions_lb,
    sensitive_features=Xlb_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_lb_1)

"""###**Part 3.** Evaluating less biased model for equalized odds

**Step 1.** Checking model fairness: Equalized Odds for Gender
"""

mf_gender_tpr_lb = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=ylb_test,
    y_pred=predictions_lb,
    sensitive_features=Xlb_test[['gender_Male', 'gender_Female']]
)

mf_gender_fpr_lb = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=ylb_test,
    y_pred=predictions_lb,
    sensitive_features=Xlb_test[['gender_Male', 'gender_Female']]
)

"""Here, the TPR for gender_Male is 0.80, and for gender_Female is 0.92. From this, we can see the model is somewhat more likely to accurately predict an income greater than $50k for individuals who identify as female than those who identify male. The TPR for both genders increased significantly from the original dataset. This change means that the model is much better at correctly predicting positive instances for this dataset than the original, for both genders."""

display(mf_gender_tpr_lb.by_group)

"""Here, the FPR for gender_Male is about 0.05, and the FPR for gender_Female is about 0.03. This means the model, trained on the given training data, is about as likely to inaccurately predict an income greater than $50k for men as for women. Both measurements show a decrease from the original dataset. This makes sense, because the some instances were duplicated, strengthening the logic used by the decision tree to minimize false predictions."""

display(mf_gender_fpr_lb.by_group)

"""The TPR gap is 0.12, which is a bit higher than the TPR gap for the original dataset. This is not unexpected, since rebalancing the data can shift the decision boundary of the model so that the TPR of each group is improved, but disproportionately, since TPR gaps depend on group-specific feature distributions and the learned decision boundary."""

mf_gender_tpr_lb.difference('between_groups')

"""The FPR gap is 0.02, lower than the FPR gap for the original dataset. This is expected because when both FPRs start out small, refactoring the dataset to decrease FPR will reduce the possible difference between them since there is a smaller possible range."""

mf_gender_fpr_lb.difference('between_groups')

"""Here, the result for equalized odds is, on average, 0.13, which is right in the middle of the fairness range (up to 0.25). This is about the same as the original data, since the TPR gap is about the same, even though the FPR gap improved."""

eo_g_lb_1 = equalized_odds_difference(
    y_true=ylb_test,
    y_pred=predictions_lb,
    sensitive_features=Xlb_test[['gender_Male', 'gender_Female']],
)

print(eo_g_lb_1)

"""**Step 2.** Checking model fairness: Predictive Parity for Race"""

mf_race_tpr_lb = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=ylb_test,
    y_pred=predictions_lb,
    sensitive_features=Xlb_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

mf_race_fpr_lb = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=ylb_test,
    y_pred=predictions_lb,
    sensitive_features=Xlb_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the TPR for race_White is 0.82, for race_Asian-Pac-Islander is 0.90, for race_Black is 0.95, for race_Amer-Indian-Eskimo is 0.86, and for race_Other is 0.89. From this, we can see the model is near as likely to accurately predict an income greater than $50k for all individuals regardless of race. These changes mean that the model is much better at correctly predicting positive instances for this dataset than the original, for all races."""

display(mf_race_tpr_lb.by_group)

"""Here, the FPR for race_White is 0.05, for race_Asian-Pac-Islander is 0.04, for race_Black is 0.03, for race_Amer-Indian-Eskimo is 0.09, and for race_Other is 0.05. From this, we can see the model is near as likely to inaccurately predict an income greater than $50k for all individuals regardless of race. These numbers show a huge decrease from the original dataset, which makes sense because data was refactored, duplicating some data instances and strengthening the logic for the decision tree."""

display(mf_race_fpr_lb.by_group)

"""The TPR gap is 0.12, which is a bit higher than the TPR gap for the original dataset. this is not unexpected, since rebalancing the data can shift the decision boundary of the model so that the TPR of each group is improved, but disproportionately, since TPR gaps depend on group-specific feature distributions and the learned decision boundary."""

mf_race_tpr_lb.difference('between_groups')

"""The FPR gap is 0.05, lower than the FPR gap for the original dataset. This is expected because when both FPRs start out small, refactoring the dataset to decrease FPR will reduce the possible difference between them since there is a smaller possible range."""

mf_race_fpr_lb.difference('between_groups')

"""Here, the result for equalized odds is, on average, 0.12, which is right in the middle of the fairness range (up to 0.25). This is about the same as the original data, since the TPR gap is about the same, even though the FPR gap improved."""

eo_r_lb_1 = equalized_odds_difference(
    y_true=ylb_test,
    y_pred=predictions_lb,
    sensitive_features=Xlb_test[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_lb_1)

"""##**Comparing Fairness Metrics from Each Version of the Dataset**

**Step 1.** Train the model and evaluate metrics multiple times to find average values
"""

original_dp_gender = [0.17, 0.19, 0.18, 0.17, 0.17]
original_dp_race = [0.11, 0.16, 0.15, 0.18, 0.13]
original_eo_gender = [0.14, 0.15, 0.12, 0.16, 0.11]
original_eo_race = [0.24, 0.28, 0.33, 0.38, 0.34]

morebias_dp_gender = [0.34, 0.36, 0.33, 0.35, 0.35]
morebias_dp_race = [0.58, 0.59, 0.59, 0.59, 0.60]
morebias_eo_gender = [0.27, 0.26, 0.29, 0.27, 0.28]
morebias_eo_race = [0.80, 0.53, 0.56, 0.65, 0.58]

lessbias_dp_gender = [0.03, 0.04, 0.02, 0.05, 0.02]
lessbias_dp_race = [0.09, 0.04, 0.06, 0.07, 0.05]
lessbias_eo_gender = [0.03, 0.07, 0.07, 0.08, 0.08]
lessbias_eo_race = [0.04, 0.16, 0.05, 0.16, 0.18]

ave_o_dp_g = sum(original_dp_gender) / len(original_dp_gender)
ave_o_dp_r = sum(original_dp_race) / len(original_dp_race)
ave_o_eo_g = sum(original_eo_gender) / len(original_eo_gender)
ave_o_eo_r = sum(original_eo_race) / len(original_eo_race)

ave_mb_dp_g = sum(morebias_dp_gender) / len(morebias_dp_gender)
ave_mb_dp_r = sum(morebias_dp_race) / len(morebias_dp_race)
ave_mb_eo_g = sum(morebias_eo_gender) / len(morebias_eo_gender)
ave_mb_eo_r = sum(morebias_eo_race) / len(morebias_eo_race)

ave_lb_dp_g = sum(lessbias_dp_gender) / len(lessbias_dp_gender)
ave_lb_dp_r = sum(lessbias_dp_race) / len(lessbias_dp_race)
ave_lb_eo_g = sum(lessbias_eo_gender) / len(lessbias_eo_gender)
ave_lb_eo_r = sum(lessbias_eo_race) / len(lessbias_eo_race)

"""**Step 2.** Store averaged results in a dataframe"""

results_df = pd.DataFrame({
    'Dataset': ['Original', 'More Biased', 'Less Biased'],
    'Demographic Parity (Gender)': [ave_o_dp_g, ave_mb_dp_g, ave_lb_dp_g],
    'Demographic Parity (Race)': [ave_o_dp_r, ave_mb_dp_r, ave_lb_dp_r],
    'Equalized Odds (Gender)': [ave_o_eo_g, ave_mb_eo_g, ave_lb_eo_g],
    'Equalized Odds (Race)': [ave_o_eo_r, ave_mb_eo_r, ave_lb_eo_r]
})

"""**Step 3.** Plot the results for visual data anlysis"""

# Transpose for easier plotting
plot_df = results_df.set_index('Dataset').T

# Plot
plot_df.plot(kind='bar', figsize=(10, 6))
plt.title("Model Metrics Across Dataset Bias Levels")
plt.ylabel("Metric Value")
plt.xticks(rotation=0)
plt.legend(title="Dataset")
plt.tight_layout()
plt.show()

"""##**In-Process Bias Mitigation: Exponentiated Gradient with Demographic Parity**

Some info about exponentiated gradient reduction
"""

from fairlearn.reductions import ExponentiatedGradient, DemographicParity

"""###Part 1: Less Biased Dataset with Gender

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

##Uncomment to run again; already done in previous section
#Xlb = less_bias_df.drop(['income'], axis=1, inplace=False)
#ylb = less_bias_df['income']

sensitive_features_Lg=Xlb[['gender_Male', 'gender_Female']]

X_train_Lg, X_test_Lg, y_train_Lg, y_test_Lg, gender_train_Lg, gender_test_Lg = train_test_split(Xlb, ylb, sensitive_features_Lg, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Lg_DP = ExponentiatedGradient(
    model,
    constraints=DemographicParity()
)

"""**Step 3.** Train the model"""

mitigator_Lg_DP.fit(X_train_Lg, y_train_Lg, sensitive_features=gender_train_Lg)

"""**Step 4.** Test the model"""

pred_Lg_DP = mitigator_Lg_DP.predict(X_test_Lg)

"""###Part 2: Less Biased Dataset with Race

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

sensitive_features_Lr=Xlb[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]

X_train_Lr, X_test_Lr, y_train_Lr, y_test_Lr, race_train_Lr, race_test_Lr = train_test_split(Xlb, ylb, sensitive_features_Lr, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Lr_DP = ExponentiatedGradient(
    model,
    constraints=DemographicParity()
)

"""**Step 3.** Train the model"""

mitigator_Lr_DP.fit(X_train_Lr, y_train_Lr, sensitive_features=race_train_Lr)

"""**Step 4.** Test the model"""

pred_Lr_DP = mitigator_Lr_DP.predict(X_test_Lr)

"""###Part 3: Originally Biased Dataset with Gender

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

##Uncomment to run again; already done in previous section
#X = unaltered_df.drop(['income'], axis=1, inplace=False)
#y = unaltered_df['income']

sensitive_features_Og=X[['gender_Male', 'gender_Female']]

X_train_Og, X_test_Og, y_train_Og, y_test_Og, gender_train_Og, gender_test_Og = train_test_split(X, y, sensitive_features_Og, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Og_DP = ExponentiatedGradient(
    model,
    constraints=DemographicParity()
)

"""**Step 3.** Train the model"""

mitigator_Og_DP.fit(X_train_Og, y_train_Og, sensitive_features=gender_train_Og)

"""**Step 4.** Test the model"""

pred_Og_DP = mitigator_Og_DP.predict(X_test_Og)

"""###Part 4: Originally Biased Dataset with Race

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

sensitive_features_Or=X[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]

X_train_Or, X_test_Or, y_train_Or, y_test_Or, race_train_Or, race_test_Or = train_test_split(X, y, sensitive_features_Or, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Or_DP = ExponentiatedGradient(
    model,
    constraints=DemographicParity()
)

"""**Step 3.** Train the model"""

mitigator_Or_DP.fit(X_train_Or, y_train_Or, sensitive_features=race_train_Or)

"""**Step 4.** Test the model"""

pred_Or_DP = mitigator_Or_DP.predict(X_test_Or)

"""###Part 5: More Biased Dataset with Gender

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

##Uncomment to run again; already done in previous section
#X = more_bias_df.drop(['income'], axis=1, inplace=False)
#y = more_bias_df['income']

sensitive_features_Mg=Xmb[['gender_Male', 'gender_Female']]

X_train_Mg, X_test_Mg, y_train_Mg, y_test_Mg, gender_train_Mg, gender_test_Mg = train_test_split(Xmb, ymb, sensitive_features_Mg, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Mg_DP = ExponentiatedGradient(
    model,
    constraints=DemographicParity()
)

"""**Step 3.** Train the model"""

mitigator_Mg_DP.fit(X_train_Mg, y_train_Mg, sensitive_features=gender_train_Mg)

"""**Step 4.** Test the model"""

pred_Mg_DP = mitigator_Mg_DP.predict(X_test_Mg)

"""###Part 6: More Biased Dataset with Race

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

sensitive_features_Mr=Xmb[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]

X_train_Mr, X_test_Mr, y_train_Mr, y_test_Mr, race_train_Mr, race_test_Mr = train_test_split(Xmb, ymb, sensitive_features_Mr, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Mr_DP = ExponentiatedGradient(
    model,
    constraints=DemographicParity()
)

"""**Step 3.** Train the model"""

mitigator_Mr_DP.fit(X_train_Mr, y_train_Mr, sensitive_features=race_train_Mr)

"""**Step 4.** Test the model"""

pred_Mr_DP = mitigator_Mr_DP.predict(X_test_Mr)

"""##**In-Process Bias Mitigation: Exponentiated Gradient with Equalized Odds**

Some info about equalized odds or something idk
"""

from fairlearn.reductions import EqualizedOdds

"""###Part 1: Less Biased Dataset with Gender

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

# #Uncomment to run again; already done in previous section
# sensitive_features_Lg=Xlb[['gender_Male', 'gender_Female']]
# X_train_Lg, X_test_Lg, y_train_Lg, y_test_Lg, gender_train_Lg, gender_test_Lg = train_test_split(Xlb, ylb, sensitive_features_Lg, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Lg_EO = ExponentiatedGradient(
    model,
    constraints=EqualizedOdds()
)

"""**Step 3.** Train the model"""

mitigator_Lg_EO.fit(X_train_Lg, y_train_Lg, sensitive_features=gender_train_Lg)

"""**Step 4.** Test the model"""

pred_Lg_EO = mitigator_Lg_EO.predict(X_test_Lg)

"""###Part 2: Less Biased Dataset with Race

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

# #Uncomment to run again; already done in previous section
# sensitive_features_Lr=Xlb[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
# X_train_Lr, X_test_Lr, y_train_Lr, y_test_Lr, race_train_Lr, race_test_Lr = train_test_split(Xlb, ylb, sensitive_features_Lr, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Lr_EO = ExponentiatedGradient(
    model,
    constraints=EqualizedOdds()
)

"""**Step 3.** Train the model"""

mitigator_Lr_EO.fit(X_train_Lr, y_train_Lr, sensitive_features=race_train_Lr)

"""**Step 4.** Test the model"""

pred_Lr_EO = mitigator_Lr_EO.predict(X_test_Lr)

"""###Part 3: Originally Biased Dataset with Gender

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

# #Uncomment to run again; already done in previous section
# sensitive_features_Og=X[['gender_Male', 'gender_Female']]
# X_train_Og, X_test_Og, y_train_Og, y_test_Og, gender_train_Og, gender_test_Og = train_test_split(X, y, sensitive_features_Og, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Og_EO = ExponentiatedGradient(
    model,
    constraints=EqualizedOdds()
)

"""**Step 3.** Train the model"""

mitigator_Og_EO.fit(X_train_Og, y_train_Og, sensitive_features=gender_train_Og)

"""**Step 4.** Test the model"""

pred_Og_EO = mitigator_Og_EO.predict(X_test_Og)

"""###Part 4: Originally Biased Dataset with Race

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

# #Uncomment to run again; already done in previous section
# sensitive_features_Or=X[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
# X_train_Or, X_test_Or, y_train_Or, y_test_Or, race_train_Or, race_test_Or = train_test_split(X, y, sensitive_features_Or, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Or_EO = ExponentiatedGradient(
    model,
    constraints=EqualizedOdds()
)

"""**Step 3.** Train the model"""

mitigator_Or_EO.fit(X_train_Or, y_train_Or, sensitive_features=race_train_Or)

"""**Step 4.** Test the model"""

pred_Or_EO = mitigator_Or_EO.predict(X_test_Or)

"""###Part 5: More Biased Dataset with Gender

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

# #Uncomment to run again; already done in previous section
# sensitive_features_Mg=Xmb[['gender_Male', 'gender_Female']]
# X_train_Mg, X_test_Mg, y_train_Mg, y_test_Mg, gender_train_Mg, gender_test_Mg = train_test_split(Xmb, ymb, sensitive_features_Mg, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Mg_EO = ExponentiatedGradient(
    model,
    constraints=EqualizedOdds()
)

"""**Step 3.** Train the model"""

mitigator_Mg_EO.fit(X_train_Mg, y_train_Mg, sensitive_features=gender_train_Mg)

"""**Step 4.** Test the model"""

pred_Mg_EO = mitigator_Mg_EO.predict(X_test_Mg)

"""###Part 6: More Biased Dataset with Race

**Step 1.** Split the data, including sensitive_features and using Xlb, ylb from previous section
"""

# #Uncomment to run again; already done in previous section
# sensitive_features_Mr=Xmb[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
# X_train_Mr, X_test_Mr, y_train_Mr, y_test_Mr, race_train_Mr, race_test_Mr = train_test_split(Xmb, ymb, sensitive_features_Mr, test_size=0.2)

"""**Step 2.** Wrap the model in the bias mitigator"""

mitigator_Mr_EO = ExponentiatedGradient(
    model,
    constraints=EqualizedOdds()
)

"""**Step 3.** Train the model"""

mitigator_Mr_EO.fit(X_train_Mr, y_train_Mr, sensitive_features=race_train_Mr)

"""**Step 4.** Test the model"""

pred_Mr_EO = mitigator_Mr_EO.predict(X_test_Mr)

"""##**Evaluating Mitigation Techniques: Exponentiated Gradient with Demographic Parity**

###**Part 1.** Less Biased Dataset: Accuracy

**Step 1.** Accuracy when mitigating for gender
"""

accuracy_Lg_DP = accuracy_score(y_test_Lg, pred_Lg_DP)
print(f"{accuracy_Lg_DP:.2%}")

"""**Step 2.** Accuracy when mitigating for race"""

accuracy_Lr_DP = accuracy_score(y_test_Lr, pred_Lr_DP)
print(f"{accuracy_Lr_DP:.2%}")

"""###**Part 2.** Less Biased Dataset: Demographic Parity

**Step 1.** Demographic parity when mitigating for gender
"""

#Create MetricFrame
mf_Less_DP_g = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Lg,
    y_pred=pred_Lg_DP,
    sensitive_features=X_test_Lg[['gender_Male', 'gender_Female']]
)

"""Here, the selection rate for gender_Male is 0.26, and the selection rate for gender_Female is 0.25. This is approximately equal to selection rates before the bias mitigation technique"""

#likelihood of positive prediction by gender
display(mf_Less_DP_g.by_group)

"""The overall selection rate is 0.26, which is very close to the overall selection rate before implementing bias mitigation."""

mf_Less_DP_g.overall

"""Here, demographic parity difference for gender is 0.008, which is extremely low, almost zero. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize demographic parity for gender is applied to a dataset that already has a very low demographic parity for gender, the technique still works but is limited by how much closer to zero it is possible to get, so only a small change is seen."""

#demographic parity for gender
dp_g_lb_2_dpg = demographic_parity_difference(
    y_true=y_test_Lg,
    y_pred=pred_Lg_DP,
    sensitive_features=X_test_Lg[['gender_Male', 'gender_Female']]
)

print(dp_g_lb_2_dpg)

"""Here, demographic parity difference for race is 0.095, which is very low, but actually higher than before. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize demographic parity for for one demographic is applied to a dataset that already has a very low demographic parity for another demographic, it may increase the demographic parity for the other demographic."""

#demographic parity for race
dp_r_lb_2_dpg = demographic_parity_difference(
    y_true=y_test_Lg,
    y_pred=pred_Lg_DP,
    sensitive_features=X_test_Lg[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_lb_2_dpg)

"""**Step 2.** Demographic parity when mitigating for race"""

#Create MetricFrame
mf_Less_DP_r = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Lr,
    y_pred=pred_Lr_DP,
    sensitive_features=X_test_Lr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the selection rate for race_Asian-Pac-Islander is 0.28, for race_White is 0.24, for race_Amer-Indian-Eskimo is 0.32, for race_Black is 0.26, and for race_Other is 0.25. For each race, this represents either no change or a very small change from before applying bias mitigation (<=0.05)."""

#likelihood of positive prediction by race
display(mf_Less_DP_r.by_group)

"""The overall selection rate is 0.24, which is very close to the overall selection rate before implementing bias mitigation."""

mf_Less_DP_r.overall

"""Here, demographic parity difference is 0.08, which is very low but actually higher than the demographic parity for race in before applying bias mitigation."""

#demographic parity for race
dp_r_lb_2_dpr = demographic_parity_difference(
    y_true=y_test_Lr,
    y_pred=pred_Lr_DP,
    sensitive_features=X_test_Lr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_lb_2_dpr)

"""Here, demographic parity difference is 0.04, which is approximately the same as the demographic parity for gender in before applying bias mitigation."""

#demographic parity for gender
dp_g_lb_2_dpr = demographic_parity_difference(
    y_true=y_test_Lr,
    y_pred=pred_Lr_DP,
    sensitive_features=X_test_Lr[['gender_Male', 'gender_Female']]
)

print(dp_g_lb_2_dpr)

"""###**Part 3.** Less Biased Dataset: Equalized Odds

**Step 1.** Equalized odds when mitigating for gender
"""

mf_tpr_Less_DP_g = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Lg,
    y_pred=pred_Lg_DP,
    sensitive_features=X_test_Lg[['gender_Male', 'gender_Female']]
)

mf_fpr_Less_DP_g = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Lg,
    y_pred=pred_Lg_DP,
    sensitive_features=X_test_Lg[['gender_Male', 'gender_Female']]
)

"""Here, the TPR for gender_Male is 0.83, which is slightly higher than before applying bias mitigation. For gender_Female, the tpr is 0.89, which is slightly lower than before applying bias mitigation."""

display(mf_tpr_Less_DP_g.by_group)

"""Here, the FPR for gender_Male is 0.07, which is slightly higher than before applying bias mitigation. For gender_Female, the fpr is 0.03, which is about the same as before applying bias mitigation."""

display(mf_fpr_Less_DP_g.by_group)

"""The TPR gap is 0.06, which is lower than the TPR gap for this dataset before bias mitigation."""

mf_tpr_Less_DP_g.difference('between_groups')

"""The FPR gap is 0.04, which is slightly higher than the FPR gap for this dataset before bias mitigation."""

mf_fpr_Less_DP_g.difference('between_groups')

"""Here, the result for equalized odds for gender is 0.06, which is lower than before implementing bias mitigation."""

#equalized odds for gender
eo_g_lb_2_dpg = equalized_odds_difference(
    y_true=y_test_Lg,
    y_pred=pred_Lg_DP,
    sensitive_features=X_test_Lg[['gender_Male', 'gender_Female']],
)

print(eo_g_lb_2_dpg)

"""Here, the result for equalized odds for race is 0.16, which is a bit higher than before implementing bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that already has low equalized odds for another demographic, it may increase the demographic parity for the other demographic."""

#equalized odds for race
eo_r_lb_2_dpg = equalized_odds_difference(
    y_true=y_test_Lg,
    y_pred=pred_Lg_DP,
    sensitive_features=X_test_Lg[['race_White', 'race_Asian-Pac-Islander', 'race_Black', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_lb_2_dpg)

"""**Step 2.** Equalized odds when mitigating for race"""

mf_tpr_Less_DP_r = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Lr,
    y_pred=pred_Lr_DP,
    sensitive_features=X_test_Lr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

mf_fpr_Less_DP_r = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Lr,
    y_pred=pred_Lr_DP,
    sensitive_features=X_test_Lr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the TPR for race_White is 0.82, for race_Asian-Pac-Islander is 0.91, for race_Black is 0.89, for race_Amer-Indian-Eskimo is 0.90, and for race_Other is 0.90. For race_Black, this represents a small decrease, for race_Amer-Indian-Eskimo, this represents a small increase, and for each other group, it represents no or almost no change."""

display(mf_tpr_Less_DP_r.by_group)

"""Here, the FPR for race_White is 0.05, for race_Asian-Pac-Islander is 0.03, for race_Black is 0.04, for race_Amer-Indian-Eskimo is 0.09, and for race_Other is 0.03. For race_Other, this represents a small decrease, and for each other group, it represents no or almost no change."""

display(mf_fpr_Less_DP_r.by_group)

"""Here, the result for equalized odds for race is 0.09, which is lower than before implementing bias mitigation."""

eo_r_lb_2_dpr = equalized_odds_difference(
    y_true=y_test_Lr,
    y_pred=pred_Lr_DP,
    sensitive_features=X_test_Lr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_lb_2_dpr)

"""Here, the result for equalized odds for gender is 0.13, which is about the same as before implementing bias mitigation."""

eo_g_lb_2_dpr = equalized_odds_difference(
    y_true=y_test_Lr,
    y_pred=pred_Lr_DP,
    sensitive_features=X_test_Lr[['gender_Male', 'gender_Female']],
)

print(eo_g_lb_2_dpr)

"""###**Part 4.** Originally Biased Dataset: Accuracy

**Step 1.** Accuracy when mitigating for gender
"""

accuracy_Og_DP = accuracy_score(y_test_Og, pred_Og_DP)
print(f"{accuracy_Og_DP:.2%}")

"""**Step 2.** Accuracy when mitigating for race"""

accuracy_Or_DP = accuracy_score(y_test_Or, pred_Or_DP)
print(f"{accuracy_Or_DP:.2%}")

"""###**Part 5.** Originally Biased Dataset: Demographic Parity

**Step 1.** Demographic parity when mitigating for gender
"""

#Create MetricFrame
mf_Original_DP_g = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Og,
    y_pred=pred_Og_DP,
    sensitive_features=X_test_Og[['gender_Male', 'gender_Female']]
)

"""Here, the selection rate for gender_Male is 0.27, which is slightly lower than before applying bias mitigation. The selection rate for gender_Female is 0.22, higher than before applying bias mitigation. Now the two values are much closer to each other."""

#likelihood of positive prediction by gender
display(mf_Original_DP_g.by_group)

"""The overall selection rate is 0.25, which is very close to the overall selection rate before implementing bias mitigation."""

mf_Original_DP_g.overall

"""Here, demographic parity difference for gender is 0.05, which is very low, and significantly lower than before applying bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize demographic parity for gender is applied to a dataset that already has a low demographic parity for gender, the technique works but possibly lowers demographic parity to a higher number than when applied to a dataset with a *very* low demographic parity already (more experimentation is required to come to a definitive conclusion)."""

#demographic parity for gender
dp_g_ob_2_dpg = demographic_parity_difference(
    y_true=y_test_Og,
    y_pred=pred_Og_DP,
    sensitive_features=X_test_Og[['gender_Male', 'gender_Female']]
)

print(dp_g_ob_2_dpg)

"""Here, demographic parity difference for race is 0.099, which is very low, and lower than before applying bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize demographic parity for for one demographic is applied to a dataset that already has a low (but not very low) demographic parity for another demographic, it may result in both demographic parities decreasing."""

#demographic parity for race
dp_r_ob_2_dpg = demographic_parity_difference(
    y_true=y_test_Og,
    y_pred=pred_Og_DP,
    sensitive_features=X_test_Og[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_ob_2_dpg)

"""**Step 2.** Demographic parity when mitigating for race"""

#Create MetricFrame
mf_Original_DP_r = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Or,
    y_pred=pred_Or_DP,
    sensitive_features=X_test_Or[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the selection rate for race_Asian-Pac-Islander is 0.28, for race_White is 0.25, for race_Amer-Indian-Eskimo is 0.23, for race_Black is 0.24, and for race_Other is 0.23. For Asian-Pac-Islander and White, this represents a very small change from before applying bias mitigation. For Amer-Indian-Eskimo, Black, and Other, this represents a increase to almost the values of the other race categories."""

#likelihood of positive prediction by race
display(mf_Original_DP_r.by_group)

"""The overall selection rate is 0.25, which is very close to the overall selection rate before implementing bias mitigation."""

mf_Original_DP_r.overall

"""Here, demographic parity difference is 0.08, which is low, and lower than the demographic parity for race in before applying bias mitigation."""

#demographic parity for race
dp_r_ob_2_dpr = demographic_parity_difference(
    y_true=y_test_Or,
    y_pred=pred_Or_DP,
    sensitive_features=X_test_Or[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_ob_2_dpr)

"""Here, demographic parity difference for gender is 0.19, which is approximately the same as the demographic parity for gender in before applying bias mitigation."""

#demographic parity for gender
dp_g_ob_2_dpr = demographic_parity_difference(
    y_true=y_test_Or,
    y_pred=pred_Or_DP,
    sensitive_features=X_test_Or[['gender_Male', 'gender_Female']]
)

print(dp_g_ob_2_dpr)

"""###**Part 6.** Originally Biased Dataset: Equalized Odds

**Step 1.** Equalized odds when mitigating for gender
"""

mf_tpr_Original_DP_g = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Og,
    y_pred=pred_Og_DP,
    sensitive_features=X_test_Og[['gender_Male', 'gender_Female']]
)

mf_fpr_Original_DP_g = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Og,
    y_pred=pred_Og_DP,
    sensitive_features=X_test_Og[['gender_Male', 'gender_Female']]
)

"""Here, the TPR for gender_Male is 0.53, and for gender_Female is 0.46. These are both about the same as before applying bias mitigation."""

display(mf_tpr_Original_DP_g.by_group)

"""Here, the FPR for gender_Male is 0.16, which is slightly lower than before applying bias mitigation. For gender_Female, the fpr is 0.07, which is about the same as or slightly higher than before applying bias mitigation."""

display(mf_fpr_Original_DP_g.by_group)

"""The TPR gap is 0.07, which is lower than the TPR gap for this dataset before bias mitigation."""

mf_tpr_Original_DP_g.difference('between_groups')

"""The FPR gap is 0.09, which is about the same as the FPR gap for this dataset before bias mitigation."""

mf_fpr_Original_DP_g.difference('between_groups')

"""Here, the result for equalized odds for gender is 0.09, which is lower than before implementing bias mitigation."""

#equalized odds for gender
eo_g_ob_2_dpg = equalized_odds_difference(
    y_true=y_test_Og,
    y_pred=pred_Og_DP,
    sensitive_features=X_test_Og[['gender_Male', 'gender_Female']],
)

print(eo_g_ob_2_dpg)

"""Here, the result for equalized odds for race is 0.28, which is outside the fairness range, but still significantly lower than before implementing bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that has high equalized odds for another demographic, it may decrease the demographic parity for that other demographic."""

#equalized odds for race
eo_r_ob_2_dpg = equalized_odds_difference(
    y_true=y_test_Og,
    y_pred=pred_Og_DP,
    sensitive_features=X_test_Og[['race_White', 'race_Asian-Pac-Islander', 'race_Black', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_ob_2_dpg)

"""**Step 2.** Equalized odds when mitigating for race"""

mf_tpr_Original_DP_r = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Or,
    y_pred=pred_Or_DP,
    sensitive_features=X_test_Or[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

mf_fpr_Original_DP_r = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Or,
    y_pred=pred_Or_DP,
    sensitive_features=X_test_Or[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the TPR for race_White is 0.55, for race_Asian-Pac-Islander is 0.52, for race_Black is 0.53, for race_Amer-Indian-Eskimo is 0.45, and for race_Other is 0.71. For race_Black and race_Other, this represents an increase, for race_Amer-Indian-Eskimo, this represents a small decrease, and for each other group, it represents no or almost no change."""

display(mf_tpr_Original_DP_r.by_group)

"""Here, the FPR for race_White is 0.15, for race_Asian-Pac-Islander is 0.15, for race_Black is 0.08, for race_Amer-Indian-Eskimo is 0.08, and for race_Other is 0.10. For race_Asian-Pas_Islander, race_Amer-Indian-Eskimo, and race_Other, this represents a decrease, and for the other two groups, it represents no or almost no change."""

display(mf_fpr_Original_DP_r.by_group)

"""The TPR gap is 0.26, which is just outside the fairness range, but still significantly lower than the TPR gap for this dataset before bias mitigation."""

mf_tpr_Original_DP_r.difference('between_groups')

"""The FPR gap is 0.07, which is slightly lower than the FPR gap for this dataset before bias mitigation."""

mf_fpr_Original_DP_r.difference('between_groups')

"""Here, the result for equalized odds for race is 0.26, which is just outside the fairness range, but still significantly lower than before bias mitigation."""

#equalized odds for race
eo_r_ob_2_dpr = equalized_odds_difference(
    y_true=y_test_Or,
    y_pred=pred_Or_DP,
    sensitive_features=X_test_Or[['race_White', 'race_Asian-Pac-Islander', 'race_Black', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_ob_2_dpr)

"""Here, the result for equalized odds for gender is 0.14, which is slightly lower than before implementing bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that has low equalized odds for another demographic, it may slighty decrease the exponentiated gradient for that other demographic."""

#equalized odds for gender
eo_g_ob_2_dpr = equalized_odds_difference(
    y_true=y_test_Or,
    y_pred=pred_Or_DP,
    sensitive_features=X_test_Or[['gender_Male', 'gender_Female']],
)

print(eo_g_ob_2_dpr)

"""###**Part 7.** More Biased Dataset: Accuracy

**Step 1.** Accuracy when mitigating for gender
"""

accuracy_Mg_DP = accuracy_score(y_test_Mg, pred_Mg_DP)
print(f"{accuracy_Mg_DP:.2%}")

"""**Step 2.** Accuracy when mitigating for race"""

accuracy_Mr_DP = accuracy_score(y_test_Mr, pred_Mr_DP)
print(f"{accuracy_Mr_DP:.2%}")

"""###**Part 8.** More Biased Dataset: Demographic Parity

**Step 1.** Demographic parity when mitigating for gender
"""

#Create MetricFrame
mf_More_DP_g = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Mg,
    y_pred=pred_Mg_DP,
    sensitive_features=X_test_Mg[['gender_Male', 'gender_Female']]
)

"""Here, the selection rate for gender_Male is 0.75, which is approximately equal to before applying bias mitigation. The selection rate for gender_Female is 0.70, which is significantly higher than before applying bias mitigation."""

#likelihood of positive prediction by gender
display(mf_More_DP_g.by_group)

"""The overall selection rate is 0.74, which is higher than the overall selection rate before implementing bias mitigation."""

mf_More_DP_g.overall

"""Here, demographic parity difference for gender is 0.05, which is low, and significantly lower than before applying bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize demographic parity for gender is applied to a dataset that has a high demographic parity for gender, it seemingly brings down demographic parity just as low as when applied to a dataset with a low demographic parity (more experimentation is required to come to a definitive conclusion)."""

#demographic parity for gender
dp_g_mb_2_dpg = demographic_parity_difference(
    y_true=y_test_Mg,
    y_pred=pred_Mg_DP,
    sensitive_features=X_test_Mg[['gender_Male', 'gender_Female']]
)

print(dp_g_mb_2_dpg)

"""Here, demographic parity difference for race is 0.51, which is quite high, and only a bit lower than before applying bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize demographic parity for for one demographic is applied to a dataset that has a high demographic parity for another demographic, it may result in no or a small decrease for the demographic with a high demographic parity."""

#demographic parity for race
dp_r_mb_2_dpg = demographic_parity_difference(
    y_true=y_test_Mg,
    y_pred=pred_Mg_DP,
    sensitive_features=X_test_Mg[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_mb_2_dpg)

"""**Step 2.** Demographic parity when mitigating for race"""

#Create MetricFrame
mf_More_DP_r = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Mr,
    y_pred=pred_Mr_DP,
    sensitive_features=X_test_Mr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the selection rate for race_Asian-Pac-Islander is 0.69, for race_White is 0.72, for race_Amer-Indian-Eskimo is 0.73, for race_Black is 0.72, and for race_Other is 0.66. For Asian-Pac-Islander and White, this represents a very small change from before applying bias mitigation. For Amer-Indian-Eskimo, Black, and Other, this represents a very significant increase to about the values of the other race categories."""

#likelihood of positive prediction by race
display(mf_More_DP_r.by_group)

"""The overall selection rate is 0.71, which is a bit higher than the overall selection rate before implementing bias mitigation."""

mf_More_DP_r.overall

"""Here, demographic parity difference is 0.07, which is low, and significantly lower than the demographic parity for race in before applying bias mitigation."""

#demographic parity for race
dp_r_mb_2_dpr = demographic_parity_difference(
    y_true=y_test_Mr,
    y_pred=pred_Mr_DP,
    sensitive_features=X_test_Mr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_mb_2_dpr)

"""Here, demographic parity difference for gender is 0.27, which is approximately the same as the demographic parity for gender in before applying bias mitigation."""

#demographic parity for gender
dp_g_mb_2_dpr = demographic_parity_difference(
    y_true=y_test_Mr,
    y_pred=pred_Mr_DP,
    sensitive_features=X_test_Mr[['gender_Male', 'gender_Female']]
)

print(dp_g_mb_2_dpr)

"""###**Part 9.** More Biased Dataset: Equalized Odds

**Step 1.** Equalized odds when mitigating for gender
"""

mf_tpr_More_DP_g = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Mg,
    y_pred=pred_Mg_DP,
    sensitive_features=X_test_Mg[['gender_Male', 'gender_Female']]
)

mf_fpr_More_DP_g = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Mg,
    y_pred=pred_Mg_DP,
    sensitive_features=X_test_Mg[['gender_Male', 'gender_Female']]
)

"""Here, the TPR for gender_Male is 0.80, which is about the same as before applying bias mitigation. The tpr gender_Female is 0.56, which is slightly higher than before applying bias mitigation."""

display(mf_tpr_More_DP_g.by_group)

"""Here, the FPR for gender_Male is 0.65, which is about the same as before applying bias mitigation. For gender_Female, the fpr is 0.41, which is higher than before applying bias mitigation."""

display(mf_fpr_More_DP_g.by_group)

"""The TPR gap is 0.24, which is lower than the TPR gap for this dataset before bias mitigation."""

mf_tpr_More_DP_g.difference('between_groups')

"""The FPR gap is 0.24, which is slightly lower than the FPR gap for this dataset before bias mitigation."""

mf_fpr_More_DP_g.difference('between_groups')

"""Here, the result for equalized odds for gender is 0.24, which is sliglt lower than before implementing bias mitigation."""

#equalized odds for gender
eo_g_mb_2_dpg = equalized_odds_difference(
    y_true=y_test_Mg,
    y_pred=pred_Mg_DP,
    sensitive_features=X_test_Mg[['gender_Male', 'gender_Female']],
)

print(eo_g_mb_2_dpg)

"""Here, the result for equalized odds for race is 0.53, which is high, but still significantly lower than before implementing bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that has high equalized odds for another demographic, it may decrease the demographic parity for that other demographic."""

#equalized odds for race
eo_r_mb_2_dpg = equalized_odds_difference(
    y_true=y_test_Mg,
    y_pred=pred_Mg_DP,
    sensitive_features=X_test_Mg[['race_White', 'race_Asian-Pac-Islander', 'race_Black', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_mb_2_dpg)

"""**Step 2.** Equalized odds when mitigating for race"""

mf_tpr_More_DP_r = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Mr,
    y_pred=pred_Mr_DP,
    sensitive_features=X_test_Mr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

mf_fpr_More_DP_r = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Mr,
    y_pred=pred_Mr_DP,
    sensitive_features=X_test_Mr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the TPR for race_White is 0.76, for race_Asian-Pac-Islander is 0.67, for race_Black is 0.38, for race_Amer-Indian-Eskimo is 0.31, and for race_Other is 0.53. For race_Black and race_Other, this represents an increase, for race_Asian-Pac-Islander, this represents a small decrease, and for each other group, it represents no or almost no change."""

display(mf_tpr_More_DP_r.by_group)

"""Here, the FPR for race_White is 0.62, for race_Asian-Pac-Islander is 0.61, for race_Black is 0.16, for race_Amer-Indian-Eskimo is 0.22, and for race_Other is 0.19. For race_Asian-Pas_Islander, this represents a decrease, for Amer-Indian-Eskimo, this represents an increase, and for the other two groups, it represents no or almost no change."""

display(mf_fpr_More_DP_r.by_group)

"""The TPR gap is 0.45, which is high, but still significantly lower than the TPR gap for this dataset before bias mitigation."""

mf_tpr_More_DP_r.difference('between_groups')

"""The FPR gap is 0.46, which is high, but lower than the FPR gap for this dataset before bias mitigation."""

mf_fpr_More_DP_r.difference('between_groups')

"""Here, the result for equalized odds for race is 0.46, which is high, but still lower than before bias mitigation."""

#equalized odds for race
eo_r_mb_2_dpr = equalized_odds_difference(
    y_true=y_test_Mr,
    y_pred=pred_Mr_DP,
    sensitive_features=X_test_Mr[['race_White', 'race_Asian-Pac-Islander', 'race_Black', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_mb_2_dpr)

"""Here, the result for equalized odds for gender is 0.32, which is slightly higher than before implementing bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that has high equalized odds for another demographic, it may not affect the equalized odds for that other demographic."""

#equalized odds for gender
eo_g_mb_2_dpr = equalized_odds_difference(
    y_true=y_test_Mr,
    y_pred=pred_Mr_DP,
    sensitive_features=X_test_Mr[['gender_Male', 'gender_Female']],
)

print(eo_g_mb_2_dpr)

"""##**Evaluating Mitigation Techniques: Exponentiated Gradient with Equalized Odds**

###**Part 1.** Less Biased Dataset: Accuracy

**Step 1.** Accuracy when mitigating for gender
"""

accuracy_Lg_EO = accuracy_score(y_test_Lg, pred_Lg_EO)
print(f"{accuracy_Lg_EO:.2%}")

"""**Step 2.** Accuracy when mitigating for race"""

accuracy_Lr_EO = accuracy_score(y_test_Lr, pred_Lr_EO)
print(f"{accuracy_Lr_EO:.2%}")

"""###**Part 2.** Less Biased Dataset: Demographic Parity

**Step 1.** Demographic parity when mitigating for gender
"""

#Create MetricFrame
mf_Less_EO_g = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Lg,
    y_pred=pred_Lg_EO,
    sensitive_features=X_test_Lg[['gender_Male', 'gender_Female']]
)

"""Here, the selection rate for gender_Male is 0.25, and the selection rate for gender_Female is also 0.25. These are both approximately equal to selection rates before the bias mitigation technique, but also closer together."""

#likelihood of positive prediction by gender
display(mf_Less_EO_g.by_group)

"""The overall selection rate is 0.25, which is very close to the overall selection rate before implementing bias mitigation."""

mf_Less_EO_g.overall

"""Here, demographic parity difference for gender is 0.004, which is extremely low, almost zero. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for gender is applied to a dataset that has a very low demographic parity for gender, the technique still works but is limited by how much closer to zero it is possible to get, so only a small change is seen."""

#demographic parity for gender
dp_g_lb_2_eog = demographic_parity_difference(
    y_true=y_test_Lg,
    y_pred=pred_Lg_EO,
    sensitive_features=X_test_Lg[['gender_Male', 'gender_Female']]
)

print(dp_g_lb_2_eog)

"""Here, demographic parity difference for race is 0.095, which is very low, but actually higher than before. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that has a very low demographic parity for another demographic, it may increase the demographic parity for the other demographic."""

#demographic parity for race
dp_r_lb_2_eog = demographic_parity_difference(
    y_true=y_test_Lg,
    y_pred=pred_Lg_EO,
    sensitive_features=X_test_Lg[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_lb_2_eog)

"""**Step 2.** Demographic parity when mitigating for race"""

#Create MetricFrame
mf_Less_EO_r = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Lr,
    y_pred=pred_Lr_EO,
    sensitive_features=X_test_Lr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the selection rate for race_Asian-Pac-Islander is 0.26, for race_White is 0.24, for race_Amer-Indian-Eskimo is 0.30, for race_Black is 0.25, and for race_Other is 0.24. For each race, this represents either no change or a very small change from before applying bias mitigation."""

#likelihood of positive prediction by race
display(mf_Less_EO_r.by_group)

"""The overall selection rate is 0.25, which is very close to the overall selection rate before implementing bias mitigation."""

mf_Less_EO_r.overall

"""Here, demographic parity difference is 0.07, which is very low but actually higher than the demographic parity for race in before applying bias mitigation."""

#demographic parity for race
dp_r_lb_2_eor = demographic_parity_difference(
    y_true=y_test_Lr,
    y_pred=pred_Lr_EO,
    sensitive_features=X_test_Lr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_lb_2_eor)

"""Here, demographic parity difference is 0.03, which is approximately the same as the demographic parity for gender in before applying bias mitigation."""

#demographic parity for gender
dp_g_lb_2_eor = demographic_parity_difference(
    y_true=y_test_Lr,
    y_pred=pred_Lr_EO,
    sensitive_features=X_test_Lr[['gender_Male', 'gender_Female']]
)

print(dp_g_lb_2_eor)

"""###**Part 3.** Less Biased Dataset: Equalized Odds

**Step 1.** Equalized odds when mitigating for gender
"""

mf_tpr_Less_EO_g = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Lg,
    y_pred=pred_Lg_EO,
    sensitive_features=X_test_Lg[['gender_Male', 'gender_Female']]
)

mf_fpr_Less_EO_g = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Lg,
    y_pred=pred_Lg_EO,
    sensitive_features=X_test_Lg[['gender_Male', 'gender_Female']]
)

"""Here, the TPR for gender_Male is 0.83, which is slightly higher than before applying bias mitigation. For gender_Female, the tpr is 0.89, which is slightly lower than before applying bias mitigation."""

display(mf_tpr_Less_DP_g.by_group)

"""Here, the FPR for gender_Male is 0.07, which is slightly higher than before applying bias mitigation. For gender_Female, the fpr is 0.03, which is about the same as before applying bias mitigation."""

display(mf_fpr_Less_DP_g.by_group)

"""The TPR gap is 0.06, which is lower than the TPR gap for this dataset before bias mitigation."""

mf_tpr_Less_DP_g.difference('between_groups')

"""The FPR gap is 0.04, which is slightly higher than the FPR gap for this dataset before bias mitigation."""

mf_fpr_Less_DP_g.difference('between_groups')

"""Here, the result for equalized odds for gender is 0.06, which is lower than before implementing bias mitigation."""

#equalized odds for gender
eo_g_lb_2_eog = equalized_odds_difference(
    y_true=y_test_Lg,
    y_pred=pred_Lg_EO,
    sensitive_features=X_test_Lg[['gender_Male', 'gender_Female']],
)

print(eo_g_lb_2_eog)

"""Here, the result for equalized odds for race is 0.16, which is a bit higher than before implementing bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that already has low equalized odds for another demographic, it may increase the demographic parity for the other demographic."""

#equalized odds for race
eo_r_lb_2_eog = equalized_odds_difference(
    y_true=y_test_Lg,
    y_pred=pred_Lg_EO,
    sensitive_features=X_test_Lg[['race_White', 'race_Asian-Pac-Islander', 'race_Black', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_lb_2_eog)

"""**Step 2.** Equalized odds when mitigating for race"""

mf_tpr_Less_DP_r = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Lr,
    y_pred=pred_Lr_EO,
    sensitive_features=X_test_Lr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

mf_fpr_Less_DP_r = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Lr,
    y_pred=pred_Lr_EO,
    sensitive_features=X_test_Lr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the TPR for race_White is 0.82, for race_Asian-Pac-Islander is 0.91, for race_Black is 0.89, for race_Amer-Indian-Eskimo is 0.90, and for race_Other is 0.90. For race_Black, this represents a small decrease, for race_Amer-Indian-Eskimo, this represents a small increase, and for each other group, it represents no or almost no change."""

display(mf_tpr_Less_DP_r.by_group)

"""Here, the FPR for race_White is 0.05, for race_Asian-Pac-Islander is 0.03, for race_Black is 0.04, for race_Amer-Indian-Eskimo is 0.09, and for race_Other is 0.03. For race_Other, this represents a small decrease, and for each other group, it represents no or almost no change."""

display(mf_fpr_Less_DP_r.by_group)

"""Here, the result for equalized odds for race is 0.09, which is lower than before implementing bias mitigation."""

eo_r_lb_2_eor = equalized_odds_difference(
    y_true=y_test_Lr,
    y_pred=pred_Lr_EO,
    sensitive_features=X_test_Lr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_lb_2_eor)

"""Here, the result for equalized odds for gender is 0.13, which is about the same as before implementing bias mitigation."""

eo_g_lb_2_eor = equalized_odds_difference(
    y_true=y_test_Lr,
    y_pred=pred_Lr_EO,
    sensitive_features=X_test_Lr[['gender_Male', 'gender_Female']],
)

print(eo_g_lb_2_eor)

"""###**Part 4.** Originally Biased Dataset: Accuracy

**Step 1.** Accuracy when mitigating for gender
"""

accuracy_Og_EO = accuracy_score(y_test_Og, pred_Og_EO)
print(f"{accuracy_Og_EO:.2%}")

"""**Step 2.** Accuracy when mitigating for race"""

accuracy_Or_EO = accuracy_score(y_test_Or, pred_Or_EO)
print(f"{accuracy_Or_EO:.2%}")

"""###**Part 5.** Originally Biased Dataset: Demographic Parity

**Step 1.** Demographic parity when mitigating for gender
"""

#Create MetricFrame
mf_Original_EO_g = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Og,
    y_pred=pred_Og_EO,
    sensitive_features=X_test_Og[['gender_Male', 'gender_Female']]
)

"""Here, the selection rate for gender_Male is 0.27, which is slightly lower than before applying bias mitigation. The selection rate for gender_Female is 0.12, about the same as before applying bias mitigation."""

#likelihood of positive prediction by gender
display(mf_Original_EO_g.by_group)

"""The overall selection rate is 0.23, about the same as the overall selection rate before implementing bias mitigation."""

mf_Original_EO_g.overall

"""Here, demographic parity difference for gender is 0.17, which is slightly lower than before applying bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for gender is applied to a dataset that has a low demographic parity for gender, the technique works a little bit."""

#demographic parity for gender
dp_g_ob_2_eog = demographic_parity_difference(
    y_true=y_test_Og,
    y_pred=pred_Og_EO,
    sensitive_features=X_test_Og[['gender_Male', 'gender_Female']]
)

print(dp_g_ob_2_eog)

"""
Here, demographic parity difference for race is 0.17, which is slightly lower than before applying bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that already has a low (but not very low) demographic parity for another demographic, it may result in a small decrease for both demographic parities."""

#demographic parity for race
dp_r_ob_2_eog = demographic_parity_difference(
    y_true=y_test_Og,
    y_pred=pred_Og_EO,
    sensitive_features=X_test_Og[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_ob_2_eog)

"""**Step 2.** Demographic parity when mitigating for race"""

#Create MetricFrame
mf_Original_EO_r = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Or,
    y_pred=pred_Or_EO,
    sensitive_features=X_test_Or[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""
Here, the selection rate for race_Asian-Pac-Islander is 0.28, for race_White is 0.25, for race_Amer-Indian-Eskimo is 0.14, for race_Black is 0.14, and for race_Other is 0.16. For each race, this represents no change or a very small change from before implementing bias mitigation."""

#likelihood of positive prediction by race
display(mf_Original_EO_r.by_group)

"""The overall selection rate is 0.24, which is very close to the overall selection rate before implementing bias mitigation."""

mf_Original_EO_r.overall

"""Here, demographic parity difference is 0.14, which is low, and lower than the demographic parity for race in before applying bias mitigation."""

#demographic parity for race
dp_r_ob_2_eor = demographic_parity_difference(
    y_true=y_test_Or,
    y_pred=pred_Or_EO,
    sensitive_features=X_test_Or[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_ob_2_eor)

"""Here, demographic parity difference for gender is 0.20, which is a bit higher than the demographic parity for gender in before applying bias mitigation."""

#demographic parity for gender
dp_g_ob_2_eor = demographic_parity_difference(
    y_true=y_test_Or,
    y_pred=pred_Or_EO,
    sensitive_features=X_test_Or[['gender_Male', 'gender_Female']]
)

print(dp_g_ob_2_eor)

"""###**Part 6.** Originally Biased Dataset: Equalized Odds

**Step 1.** Equalized odds when mitigating for gender
"""

mf_tpr_Original_DP_g = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Og,
    y_pred=pred_Og_EO,
    sensitive_features=X_test_Og[['gender_Male', 'gender_Female']]
)

mf_fpr_Original_DP_g = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Og,
    y_pred=pred_Og_EO,
    sensitive_features=X_test_Og[['gender_Male', 'gender_Female']]
)

"""Here, the TPR for gender_Male is 0.53, and for gender_Female is 0.46. These are both about the same as before applying bias mitigation."""

display(mf_tpr_Original_DP_g.by_group)

"""Here, the FPR for gender_Male is 0.16, which is slightly lower than before applying bias mitigation. For gender_Female, the fpr is 0.07, which is about the same as or slightly higher than before applying bias mitigation."""

display(mf_fpr_Original_DP_g.by_group)

"""The TPR gap is 0.07, which is lower than the TPR gap for this dataset before bias mitigation."""

mf_tpr_Original_DP_g.difference('between_groups')

"""The FPR gap is 0.09, which is about the same as the FPR gap for this dataset before bias mitigation."""

mf_fpr_Original_DP_g.difference('between_groups')

"""Here, the result for equalized odds for gender is 0.09, which is lower than before implementing bias mitigation."""

#equalized odds for gender
eo_g_ob_2_eog = equalized_odds_difference(
    y_true=y_test_Og,
    y_pred=pred_Og_EO,
    sensitive_features=X_test_Og[['gender_Male', 'gender_Female']],
)

print(eo_g_ob_2_eog)

"""Here, the result for equalized odds for race is 0.28, which is outside the fairness range, but still significantly lower than before implementing bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that has high equalized odds for another demographic, it may decrease the demographic parity for that other demographic."""

#equalized odds for race
eo_r_ob_2_eog = equalized_odds_difference(
    y_true=y_test_Og,
    y_pred=pred_Og_EO,
    sensitive_features=X_test_Og[['race_White', 'race_Asian-Pac-Islander', 'race_Black', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_ob_2_eog)

"""**Step 2.** Equalized odds when mitigating for race"""

mf_tpr_Original_DP_r = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Or,
    y_pred=pred_Or_EO,
    sensitive_features=X_test_Or[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

mf_fpr_Original_DP_r = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Or,
    y_pred=pred_Or_EO,
    sensitive_features=X_test_Or[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the TPR for race_White is 0.55, for race_Asian-Pac-Islander is 0.52, for race_Black is 0.53, for race_Amer-Indian-Eskimo is 0.45, and for race_Other is 0.71. For race_Black and race_Other, this represents an increase, for race_Amer-Indian-Eskimo, this represents a small decrease, and for each other group, it represents no or almost no change."""

display(mf_tpr_Original_DP_r.by_group)

"""Here, the FPR for race_White is 0.15, for race_Asian-Pac-Islander is 0.15, for race_Black is 0.08, for race_Amer-Indian-Eskimo is 0.08, and for race_Other is 0.10. For race_Asian-Pas_Islander, race_Amer-Indian-Eskimo, and race_Other, this represents a decrease, and for the other two groups, it represents no or almost no change."""

display(mf_fpr_Original_DP_r.by_group)

"""The TPR gap is 0.26, which is just outside the fairness range, but still significantly lower than the TPR gap for this dataset before bias mitigation."""

mf_tpr_Original_DP_r.difference('between_groups')

"""The FPR gap is 0.07, which is slightly lower than the FPR gap for this dataset before bias mitigation."""

mf_fpr_Original_DP_r.difference('between_groups')

"""Here, the result for equalized odds for race is 0.26, which is just outside the fairness range, but still significantly lower than before bias mitigation."""

#equalized odds for race
eo_r_ob_2_eor = equalized_odds_difference(
    y_true=y_test_Or,
    y_pred=pred_Or_EO,
    sensitive_features=X_test_Or[['race_White', 'race_Asian-Pac-Islander', 'race_Black', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_ob_2_eor)

"""Here, the result for equalized odds for gender is 0.14, which is slightly lower than before implementing bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that has low equalized odds for another demographic, it may slighty decrease the exponentiated gradient for that other demographic."""

#equalized odds for gender
eo_g_ob_2_eor = equalized_odds_difference(
    y_true=y_test_Or,
    y_pred=pred_Or_EO,
    sensitive_features=X_test_Or[['gender_Male', 'gender_Female']],
)

print(eo_g_ob_2_eor)

"""###**Part 7.** More Biased Dataset: Accuracy

**Step 1.** Accuracy when mitigating for gender
"""

accuracy_Mg_EO = accuracy_score(y_test_Mg, pred_Mg_EO)
print(f"{accuracy_Mg_EO:.2%}")

"""**Step 2.** Accuracy when mitigating for race"""

accuracy_Mr_EO = accuracy_score(y_test_Mr, pred_Mr_EO)
print(f"{accuracy_Mr_EO:.2%}")

"""###**Part 8.** More Biased Dataset: Demographic Parity

**Step 1.** Demographic parity when mitigating for gender
"""

#Create MetricFrame
mf_More_EO_g = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Mg,
    y_pred=pred_Mg_EO,
    sensitive_features=X_test_Mg[['gender_Male', 'gender_Female']]
)

"""Here, the selection rate for gender_Male is 0.77, which is approximately equal to before applying bias mitigation. The selection rate for gender_Female is 0.48, which is a bit higher than before applying bias mitigation."""

#likelihood of positive prediction by gender
display(mf_More_EO_g.by_group)

"""The overall selection rate is 0.68, which is slightly higher than the overall selection rate before implementing bias mitigation."""

mf_More_EO_g.overall

"""Here, demographic parity difference for gender is 0.30, which is about the same as before applying bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for gender is applied to a dataset that has a high demographic parity for gender, it seemingly does not change demographic parity."""

#demographic parity for gender
dp_g_mb_2_eog = demographic_parity_difference(
    y_true=y_test_Mg,
    y_pred=pred_Mg_EO,
    sensitive_features=X_test_Mg[['gender_Male', 'gender_Female']]
)

print(dp_g_mb_2_eog)

"""Here, demographic parity difference for race is 0.60, which is about the same as before applying bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that has a high demographic parity for another demographic, it may result in no change for that other demographic."""

#demographic parity for race
dp_r_mb_2_eog = demographic_parity_difference(
    y_true=y_test_Mg,
    y_pred=pred_Mg_EO,
    sensitive_features=X_test_Mg[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_mb_2_eog)

"""**Step 2.** Demographic parity when mitigating for race"""

#Create MetricFrame
mf_More_EO_r = MetricFrame(
    metrics={'selection rate' : selection_rate},
    y_true=y_test_Mr,
    y_pred=pred_Mr_EO,
    sensitive_features=X_test_Mr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the selection rate for race_Asian-Pac-Islander is 0.65, for race_White is 0.73, for race_Amer-Indian-Eskimo is 0.23, for race_Black is 0.19, and for race_Other is 0.26. For Amer-Indian-Eskimo and Other, this an increase from before applying bias mitigation. For the other races this represents approximately no change to the values of the other race categories."""

#likelihood of positive prediction by race
display(mf_More_EO_r.by_group)

"""The overall selection rate is 0.67, which is about the same as the overall selection rate before implementing bias mitigation."""

mf_More_EO_r.overall

"""Here, demographic parity difference is 0.53, which is high, and slightly lower than the demographic parity for race in before applying bias mitigation."""

#demographic parity for race
dp_r_mb_2_eor = demographic_parity_difference(
    y_true=y_test_Mr,
    y_pred=pred_Mr_EO,
    sensitive_features=X_test_Mr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

print(dp_r_mb_2_eor)

"""Here, demographic parity difference for gender is 0.35, which is approximately the same as or a bit higher than the demographic parity for gender in before applying bias mitigation."""

#demographic parity for gender
dp_g_mb_2_eor = demographic_parity_difference(
    y_true=y_test_Mr,
    y_pred=pred_Mr_EO,
    sensitive_features=X_test_Mr[['gender_Male', 'gender_Female']]
)

print(dp_g_mb_2_eor)

"""###**Part 9.** More Biased Dataset: Equalized Odds

**Step 1.** Equalized odds when mitigating for gender
"""

mf_tpr_More_DP_g = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Mg,
    y_pred=pred_Mg_EO,
    sensitive_features=X_test_Mg[['gender_Male', 'gender_Female']]
)

mf_fpr_More_DP_g = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Mg,
    y_pred=pred_Mg_EO,
    sensitive_features=X_test_Mg[['gender_Male', 'gender_Female']]
)

"""Here, the TPR for gender_Male is 0.80, which is about the same as before applying bias mitigation. The tpr gender_Female is 0.56, which is slightly higher than before applying bias mitigation."""

display(mf_tpr_More_DP_g.by_group)

"""Here, the FPR for gender_Male is 0.65, which is about the same as before applying bias mitigation. For gender_Female, the fpr is 0.41, which is higher than before applying bias mitigation."""

display(mf_fpr_More_DP_g.by_group)

"""The TPR gap is 0.24, which is lower than the TPR gap for this dataset before bias mitigation."""

mf_tpr_More_DP_g.difference('between_groups')

"""The FPR gap is 0.24, which is slightly lower than the FPR gap for this dataset before bias mitigation."""

mf_fpr_More_DP_g.difference('between_groups')

"""Here, the result for equalized odds for gender is 0.24, which is sliglt lower than before implementing bias mitigation."""

#equalized odds for gender
eo_g_mb_2_eog = equalized_odds_difference(
    y_true=y_test_Mg,
    y_pred=pred_Mg_EO,
    sensitive_features=X_test_Mg[['gender_Male', 'gender_Female']],
)

print(eo_g_mb_2_eog)

"""Here, the result for equalized odds for race is 0.53, which is high, but still significantly lower than before implementing bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that has high equalized odds for another demographic, it may decrease the demographic parity for that other demographic."""

#equalized odds for race
eo_r_mb_2_eog = equalized_odds_difference(
    y_true=y_test_Mg,
    y_pred=pred_Mg_EO,
    sensitive_features=X_test_Mg[['race_White', 'race_Asian-Pac-Islander', 'race_Black', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_mb_2_eog)

"""**Step 2.** Equalized odds when mitigating for race"""

mf_tpr_More_DP_r = MetricFrame(
    metrics={'tpr': lambda y_t, y_p: recall_score(y_t, y_p)},
    y_true=y_test_Mr,
    y_pred=pred_Mr_EO,
    sensitive_features=X_test_Mr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

mf_fpr_More_DP_r = MetricFrame(
    metrics={'fpr': lambda y_t, y_p: fpr(y_t, y_p)},
    y_true=y_test_Mr,
    y_pred=pred_Mr_EO,
    sensitive_features=X_test_Mr[['race_White', 'race_Black', 'race_Asian-Pac-Islander', 'race_Amer-Indian-Eskimo', 'race_Other']]
)

"""Here, the TPR for race_White is 0.76, for race_Asian-Pac-Islander is 0.67, for race_Black is 0.38, for race_Amer-Indian-Eskimo is 0.31, and for race_Other is 0.53. For race_Black and race_Other, this represents an increase, for race_Asian-Pac-Islander, this represents a small decrease, and for each other group, it represents no or almost no change."""

display(mf_tpr_More_DP_r.by_group)

"""Here, the FPR for race_White is 0.62, for race_Asian-Pac-Islander is 0.61, for race_Black is 0.16, for race_Amer-Indian-Eskimo is 0.22, and for race_Other is 0.19. For race_Asian-Pas_Islander, this represents a decrease, for Amer-Indian-Eskimo, this represents an increase, and for the other two groups, it represents no or almost no change."""

display(mf_fpr_More_DP_r.by_group)

"""The TPR gap is 0.45, which is high, but still significantly lower than the TPR gap for this dataset before bias mitigation."""

mf_tpr_More_DP_r.difference('between_groups')

"""The FPR gap is 0.46, which is high, but lower than the FPR gap for this dataset before bias mitigation."""

mf_fpr_More_DP_r.difference('between_groups')

"""Here, the result for equalized odds for race is 0.46, which is high, but still lower than before bias mitigation."""

#equalized odds for race
eo_r_mb_2_eor = equalized_odds_difference(
    y_true=y_test_Mr,
    y_pred=pred_Mr_EO,
    sensitive_features=X_test_Mr[['race_White', 'race_Asian-Pac-Islander', 'race_Black', 'race_Amer-Indian-Eskimo', 'race_Other']],
)

print(eo_r_mb_2_eor)

"""Here, the result for equalized odds for gender is 0.32, which is slightly higher than before implementing bias mitigation. This shows us that when the bias mitigation technique of exponentiated gradient optimized to minimize equalized odds for for one demographic is applied to a dataset that has high equalized odds for another demographic, it may not affect the equalized odds for that other demographic."""

#equalized odds for gender
eo_g_mb_2_eor = equalized_odds_difference(
    y_true=y_test_Mr,
    y_pred=pred_Mr_EO,
    sensitive_features=X_test_Mr[['gender_Male', 'gender_Female']],
)

print(eo_g_mb_2_eor)

"""##**Comparing Bias Mitigation Techniques-- Final Results**

###**Part 1.** Compile results into lists for analysis

Lists to display the results of the Exponentiated Gradient with Demographic Parity for Gender (EGDPG) bias mitigation technique
"""

#Demographic Parity_ of Gender and Race_ for each dataset_ before_ bias mitigation_
dp_g = [dp_g_lb_1, dp_g_ob_1, dp_g_mb_1]
dp_r = [dp_r_lb_1, dp_r_ob_1, dp_r_mb_1]

#Demographic Parity_ of Gender and Race_ for each dataset_ after_ Exponentiated Gradient with Demographic Parity for Gender bias mitigation_
dp_egdpg_g = [dp_g_lb_2_dpg, dp_g_ob_2_dpg, dp_g_mb_2_dpg]
dp_egdpg_r = [dp_r_lb_2_dpg, dp_r_ob_2_dpg, dp_r_mb_2_dpg]

#Equalized Odds_ of Gender and Race_ for each dataset_ before_ bias mitigation_
eo_g = [eo_g_lb_1, eo_g_ob_1, eo_g_mb_1]
eo_r = [eo_r_lb_1, eo_r_ob_1, eo_r_mb_1]

#Equalized Odds_ of Gender and Race_ for each dataset_ after_ Exponentiated Gradient with Demographic Parity for Gender bias mitigation_
eo_egdpg_g = [eo_g_lb_2_dpg, eo_g_ob_2_dpg, eo_g_mb_2_dpg]
eo_egdpg_r = [eo_r_lb_2_dpg, eo_r_ob_2_dpg, eo_r_mb_2_dpg]

"""Lists to display the results of the Exponentiated Gradient with Demographic Parity for Race (EGDPR) bias mitigation technique"""

#Demographic Parity_ of Gender and Race_ for each dataset_ before_ bias mitigation_
#dp_1

#Demographic Parity_ of Gender and Race_ for each dataset_ after_ Exponentiated Gradient with Demographic Parity for Race bias mitigation_
dp_egdpr_g = [dp_g_lb_2_dpr, dp_g_ob_2_dpr, dp_g_mb_2_dpr]
dp_egdpr_r = [dp_r_lb_2_dpr, dp_r_ob_2_dpr, dp_r_mb_2_dpr]

#Equalized Odds_ of Gender and Race_ for each dataset_ before_ bias mitigation_
#eo_1

#Equalized Odds_ of Gender and Race_ for each dataset_ after_ Exponentiated Gradient with Demographic Parity for Race bias mitigation_
eo_egdpr_g = [eo_g_lb_2_dpr, eo_g_ob_2_dpr, eo_g_mb_2_dpr]
eo_egdpr_r = [eo_r_lb_2_dpr, eo_r_ob_2_dpr, eo_r_mb_2_dpr]

"""Lists to display the results of the Exponentiated Gradient with Equalized Odds for Gender (EGEOG) bias mitigation technique"""

#Demographic Parity_ of Gender and Race_ for each dataset_ before_ bias mitigation_
#dp_1

#Demographic Parity_ of Gender and Race_ for each dataset_ after_ Exponentiated Gradient with Equalized Odds for Gender bias mitigation_
dp_egeog_g = [dp_g_lb_2_eog, dp_g_ob_2_eog, dp_g_mb_2_eog]
dp_egeog_r = [dp_r_lb_2_eog, dp_r_ob_2_eog, dp_r_mb_2_eog]

#Equalized Odds_ of Gender and Race_ for each dataset_ before_ bias mitigation_
#eo_1

#Equalized Odds_ of Gender and Race_ for each dataset_ after_ Exponentiated Gradient with Equalized Odds for Gender bias mitigation_
eo_egeog_g = [eo_g_lb_2_eog, eo_g_ob_2_eog, eo_g_mb_2_eog]
eo_egeog_r = [eo_r_lb_2_eog, eo_r_ob_2_eog, eo_r_mb_2_eog]

"""Lists to display the results of the Exponentiated Gradient with Equalized Odds for Race (EGEOR) bias mitigation technique"""

#Demographic Parity_ of Gender and Race_ for each dataset_ before_ bias mitigation_
#dp_1

#Demographic Parity_ of Gender and Race_ for each dataset_ after_ Exponentiated Gradient with Equalized Odds for Race bias mitigation_
dp_egeor_g = [dp_g_lb_2_eor, dp_g_ob_2_eor, dp_g_mb_2_eor]
dp_egeor_r = [dp_r_lb_2_eor, dp_r_ob_2_eor, dp_r_mb_2_eor]

#Equalized Odds_ of Gender and Race_ for each dataset_ before_ bias mitigation_
#eo_1

#Equalized Odds_ of Gender and Race_ for each dataset_ after_ Exponentiated Gradient with Equalized Odds for Race bias mitigation_
eo_egeor_g = [eo_g_lb_2_eor, eo_g_ob_2_eor, eo_g_mb_2_eor]
eo_egeor_r = [eo_r_lb_2_eor, eo_r_ob_2_eor, eo_r_mb_2_eor]

lb_dp_gender = [dp_g_lb_1, dp_g_lb_2_dpg, dp_g_lb_2_dpr, dp_g_lb_2_eog, dp_g_lb_2_dpr]
lb_dp_race = [dp_r_lb_1, dp_r_lb_2_dpg, dp_r_lb_2_dpr, dp_r_lb_2_eog, dp_r_lb_2_eor]
lb_eo_gender = [eo_g_lb_1, eo_g_lb_2_dpg, eo_g_lb_2_dpr, eo_g_lb_2_eog, eo_g_lb_2_eor]
lb_eo_race = [eo_r_lb_1, eo_r_lb_2_dpg, eo_r_lb_2_dpr, eo_r_lb_2_eog, eo_r_lb_2_eor]

ob_dp_gender = [dp_g_ob_1, dp_g_ob_2_dpg, dp_g_ob_2_dpr, dp_g_ob_2_eog, dp_g_ob_2_dpr]
ob_dp_race = [dp_r_ob_1, dp_r_ob_2_dpg, dp_r_ob_2_dpr, dp_r_ob_2_eog, dp_r_ob_2_eor]
ob_eo_gender = [eo_g_ob_1, eo_g_ob_2_dpg, eo_g_ob_2_dpr, eo_g_ob_2_eog, eo_g_ob_2_eor]
ob_eo_race = [eo_r_ob_1, eo_r_ob_2_dpg, eo_r_ob_2_dpr, eo_r_ob_2_eog, eo_r_ob_2_eor]

mb_dp_gender = [dp_g_mb_1, dp_g_mb_2_dpg, dp_g_mb_2_dpr, dp_g_mb_2_eog, dp_g_mb_2_dpr]
mb_dp_race = [dp_r_mb_1, dp_r_mb_2_dpg, dp_r_mb_2_dpr, dp_r_mb_2_eog, dp_r_mb_2_eor]
mb_eo_gender = [eo_g_mb_1, eo_g_mb_2_dpg, eo_g_mb_2_dpr, eo_g_mb_2_eog, eo_g_mb_2_eor]
mb_eo_race = [eo_r_mb_1, eo_r_mb_2_dpg, eo_r_mb_2_dpr, eo_r_mb_2_eog, eo_r_mb_2_eor]

"""Lists to display the demographic parity results of each bias mitigation technique for the Less Biased dataset"""

#Demographic Parity_ of Gender_ for the Less Biased dataset_ before and after_ each bias mitigation technique_
dp_g_lb =[dp_g_lb_1, dp_g_lb_2_dpg, dp_g_lb_2_dpr, dp_g_lb_2_eog, dp_g_lb_2_eor]

#Demographic Parity_ of Race_ for the Less Biased dataset_ before and after each bias mitigation technique_
dp_r_lb =[dp_r_lb_1, dp_r_lb_2_dpg, dp_r_lb_2_dpr, dp_r_lb_2_eog, dp_r_lb_2_eor]

"""Lists to display the demographic parity results of each bias mitigation technique for the Originally Biased dataset"""

#Demographic Parity_ of Gender_ for the Originally Biased dataset_ after_ each bias mitigation technique_
dp_g_ob =[dp_g_ob_1, dp_g_ob_2_dpg, dp_g_ob_2_dpr, dp_g_ob_2_eog, dp_g_ob_2_eor]

#Demographic Parity_ of Race_ for the Originally Biased dataset_ after each bias mitigation technique_
dp_r_ob =[dp_r_ob_1, dp_r_ob_2_dpg, dp_r_ob_2_dpr, dp_r_ob_2_eog, dp_r_ob_2_eor]

"""Lists to display the demographic parity results of each bias mitigation technique for the More Biased dataset"""

#Demographic Parity_ of Gender_ for the More Biased dataset_ after_ each bias mitigation technique_
dp_g_mb =[dp_g_mb_1, dp_g_mb_2_dpg, dp_g_mb_2_dpr, dp_g_mb_2_eog, dp_g_mb_2_eor]

#Demographic Parity_ of Race_ for the More Biased dataset_ before_ each bias mitigation technique_
dp_r_mb =[dp_r_mb_1, dp_r_mb_2_dpg, dp_r_mb_2_dpr, dp_r_mb_2_eog, dp_r_mb_2_eor]

"""Lists to display the equalized odds results of each bias mitigation technique for the Less Biased dataset"""

#Equalized Odds_ of Gender_ for the Less Biased dataset_ after_ each bias mitigation technique_
eo_g_lb =[eo_g_lb_1, eo_g_lb_2_dpg, eo_g_lb_2_dpr, eo_g_lb_2_eog, eo_g_lb_2_eor]

#Equalized Odds_ of Race_ for the Less Biased dataset_ after each bias mitigation technique_
eo_r_lb =[eo_r_lb_1, eo_r_lb_2_dpg, eo_r_lb_2_dpr, eo_r_lb_2_eog, eo_r_lb_2_eor]

"""Lists to display the equalized odds results of each bias mitigation technique for the Originally Biased dataset"""

#Equalized Odds_ of Gender_ for the Originally Biased dataset_ after_ each bias mitigation technique_
eo_g_ob =[eo_g_ob_1, eo_g_ob_2_dpg, eo_g_ob_2_dpr, eo_g_ob_2_eog, eo_g_ob_2_eor]

#Equalized Odds_ of Race_ for the Originally Biased dataset_ after each bias mitigation technique_
eo_r_ob =[eo_r_ob_1, eo_r_ob_2_dpg, eo_r_ob_2_dpr, eo_r_ob_2_eog, eo_r_ob_2_eor]

"""Lists to display the equalized odds results of each bias mitigation technique for the More Biased dataset"""

#Equalized Odds_ of Gender_ for the More Biased dataset_ after_ each bias mitigation technique_
eo_g_mb =[eo_g_mb_1, eo_g_mb_2_dpg, eo_g_mb_2_dpr, eo_g_mb_2_eog, eo_g_mb_2_eor]

#Equalized Odds_ of Race_ for the More Biased dataset_ after each bias mitigation technique_
eo_r_mb =[eo_r_mb_1, eo_r_mb_2_dpg, eo_r_mb_2_dpr, eo_r_mb_2_eog, eo_r_mb_2_eor]

"""Lists to display accuracy"""

acc_lb = [accuracy_lb, accuracy_Lg_DP, accuracy_Lr_DP, accuracy_Lg_EO, accuracy_Lr_EO]
acc_ob = [accuracy, accuracy_Og_DP, accuracy_Or_DP, accuracy_Og_EO, accuracy_Or_EO]
acc_mb = [accuracy_mb, accuracy_Mg_DP, accuracy_Mr_DP, accuracy_Mg_EO, accuracy_Mr_EO]

acc_1 = [accuracy_lb, accuracy, accuracy_mb]
acc_egdpg = [accuracy_Lg_DP, accuracy_Og_DP, accuracy_Mg_DP]
acc_egdpr = [accuracy_Lr_DP, accuracy_Or_DP, accuracy_Mr_DP]
acc_egeog = [accuracy_Lg_EO, accuracy_Og_EO, accuracy_Mg_EO]
acc_egeor = [accuracy_Lr_EO, accuracy_Or_EO, accuracy_Mr_EO]

lb_acc = [accuracy_lb, accuracy_Lg_DP, accuracy_Lr_DP, accuracy_Lg_EO, accuracy_Lr_EO]
ob_acc = [accuracy, accuracy_Og_DP, accuracy_Or_DP, accuracy_Og_EO, accuracy_Or_EO]
mb_acc = [accuracy_mb, accuracy_Mg_DP, accuracy_Mr_DP, accuracy_Mg_EO, accuracy_Mr_EO]

"""###**Part 2.** Data Visualization: Results for Each Bias Mitigation Techniques

Comparing Impact of Bias Mitigation Techniques on Each Fairness Metric (Demographic Parity for Gender and Race, Equalized Odds for Gender and Race)
"""

labels = ["None", "DP_G", "DP_R", "EO_G", "EO_R"]

x_positions = range(len(labels))

fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)

#Demographic Parity of Gender (top left)
axes[0, 0].scatter(x_positions, dp_g_lb, color='blue', marker='^', label='Less Bias')
axes[0, 0].plot(x_positions, dp_g_lb, color='blue', linestyle='--')
axes[0, 0].scatter(x_positions, dp_g_ob, color='green', marker='o', label='Original Bias')
axes[0, 0].plot(x_positions, dp_g_ob, color='green', linestyle='--')
axes[0, 0].scatter(x_positions, dp_g_mb, color='red', marker='s', label='More Bias')
axes[0, 0].plot(x_positions, dp_g_mb, color='red', linestyle='--')

axes[0, 0].set_xticks(x_positions)
axes[0, 0].set_xticklabels(labels)
axes[0, 0].set_ylabel('Value')
axes[0, 0].set_title('Demographic Parity of Gender')
axes[0, 0].legend()

#Demographic Parity of Race (top right)
axes[0, 1].scatter(x_positions, dp_r_lb, color='blue', marker='^', label='Less Bias')
axes[0, 1].plot(x_positions, dp_r_lb, color='blue', linestyle='--')
axes[0, 1].scatter(x_positions, dp_r_ob, color='green', marker='o', label='Original Bias')
axes[0, 1].plot(x_positions, dp_r_ob, color='green', linestyle='--')
axes[0, 1].scatter(x_positions, dp_r_mb, color='red', marker='s', label='More Bias')
axes[0, 1].plot(x_positions, dp_r_mb, color='red', linestyle='--')

axes[0, 1].set_xticks(x_positions)
axes[0, 1].set_xticklabels(labels)
axes[0, 1].set_title('Demographic Parity of Race')
axes[0, 1].legend()

#Equalized Odds of Gender (bottom left)
axes[1, 0].scatter(x_positions, eo_g_lb, color='blue', marker='^', label='Less Bias')
axes[1, 0].plot(x_positions, eo_g_lb, color='blue', linestyle='--')
axes[1, 0].scatter(x_positions, eo_g_ob, color='green', marker='o', label='Original Bias')
axes[1, 0].plot(x_positions, eo_g_ob, color='green', linestyle='--')
axes[1, 0].scatter(x_positions, eo_g_mb, color='red', marker='s', label='More Bias')
axes[1, 0].plot(x_positions, eo_g_mb, color='red', linestyle='--')

axes[1, 0].set_xticks(x_positions)
axes[1, 0].set_xticklabels(labels)
axes[1, 0].set_xlabel('Bias Mitigation Techniques')
axes[1, 0].set_ylabel('Value')
axes[1, 0].set_title('Equalized Odds of Gender')
axes[1, 0].legend()

#Equalized Odds of Race (bottom right)
axes[1, 1].scatter(x_positions, eo_r_lb, color='blue', marker='^', label='Less Bias')
axes[1, 1].plot(x_positions, eo_r_lb, color='blue', linestyle='--')
axes[1, 1].scatter(x_positions, eo_r_ob, color='green', marker='o', label='Original Bias')
axes[1, 1].plot(x_positions, eo_r_ob, color='green', linestyle='--')
axes[1, 1].scatter(x_positions, eo_r_mb, color='red', marker='s', label='More Bias')
axes[1, 1].plot(x_positions, eo_r_mb, color='red', linestyle='--')

axes[1, 1].set_xticks(x_positions)
axes[1, 1].set_xticklabels(labels)
axes[1, 1].set_xlabel('Bias Mitigation Techniques')
axes[1, 1].set_title('Equalized Odds of Race')
axes[1, 1].legend()

plt.tight_layout()
plt.show()

"""Comparing Impact of Bias Mitigation Techniques on Accuracy"""

labels = ["None", "DP_G", "DP_R", "EO_G", "EO_R"]

x_positions = range(len(labels))

#Accuracy
plt.scatter(x_positions, acc_lb, color='blue', marker='^', label='Less Bias')
plt.plot(x_positions, acc_lb, color='blue', linestyle='--')
plt.scatter(x_positions, acc_ob, color='green', marker='o', label='Original Bias')
plt.plot(x_positions, acc_ob, color='green', linestyle='--')
plt.scatter(x_positions, acc_mb, color='red', marker='s', label='More Bias')
plt.plot(x_positions, acc_mb, color='red', linestyle='--')

plt.xticks(x_positions, labels)
plt.ylabel('Accuracy')
plt.xlabel('Bias Mitigation Techniques')
plt.title('Model Accuracy by Bias Mitigation Technique')
plt.legend()
plt.show()

"""Comparing Impact of Bias in the Dataset on Each Fairness Metric (Demographic Parity for Gender and Race, Equalized Odds for Gender and Race)"""

labels = ["Less Bias", "Original", "More Bias"]

x_positions = range(len(labels))

fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)

#Demographic Parity of Gender (top left)
axes[0, 0].scatter(x_positions, dp_g, color='purple', marker='^', label='None')
axes[0, 0].plot(x_positions, dp_g, color='purple', linestyle='--', alpha=0.3)
axes[0, 0].scatter(x_positions, dp_egdpg_g, color='red', marker='o', label='DP_G')
axes[0, 0].plot(x_positions, dp_egdpg_g, color='red', linestyle='--', alpha=0.3)
axes[0, 0].scatter(x_positions, dp_egdpr_g, color='orange', marker='o', label='DP_R')
axes[0, 0].plot(x_positions, dp_egdpr_g, color='orange', linestyle='--', alpha=0.3)
axes[0, 0].scatter(x_positions, dp_egeog_g, color='green', marker='s', label='EO_G')
axes[0, 0].plot(x_positions, dp_egeog_g, color='green', linestyle='--', alpha=0.3)
axes[0, 0].scatter(x_positions, dp_egeor_g, color='blue', marker='s', label='EO_R')
axes[0, 0].plot(x_positions, dp_egeor_g, color='blue', linestyle='--', alpha=0.3)

axes[0, 0].set_xticks(x_positions)
axes[0, 0].set_xticklabels(labels)
axes[0, 0].set_ylabel('Value')
axes[0, 0].set_title('Demographic Parity of Gender')
axes[0, 0].legend()

#Demographic Parity of Race (top right)
axes[0, 1].scatter(x_positions, dp_r, color='purple', marker='^', label='None')
axes[0, 1].plot(x_positions, dp_r, color='purple', linestyle='--', alpha=0.3)
axes[0, 1].scatter(x_positions, dp_egdpg_r, color='red', marker='o', label='DP_G')
axes[0, 1].plot(x_positions, dp_egdpg_r, color='red', linestyle='--', alpha=0.3)
axes[0, 1].scatter(x_positions, dp_egdpr_r, color='orange', marker='o', label='DP_R')
axes[0, 1].plot(x_positions, dp_egdpr_r, color='orange', linestyle='--', alpha=0.3)
axes[0, 1].scatter(x_positions, dp_egeog_r, color='green', marker='s', label='EO_G')
axes[0, 1].plot(x_positions, dp_egeog_r, color='green', linestyle='--', alpha=0.3)
axes[0, 1].scatter(x_positions, dp_egeor_r, color='blue', marker='s', label='EO_R')
axes[0, 1].plot(x_positions, dp_egeor_r, color='blue', linestyle='--', alpha=0.3)

axes[0, 1].set_xticks(x_positions)
axes[0, 1].set_xticklabels(labels)
axes[0, 1].set_title('Demographic Parity of Race')
axes[0, 1].legend()

#Equalized Odds of Gender (bottom left)
axes[1, 0].scatter(x_positions, eo_g, color='purple', marker='^', label='None')
axes[1, 0].plot(x_positions, eo_g, color='purple', linestyle='--', alpha=0.3)
axes[1, 0].scatter(x_positions, eo_egdpg_g, color='red', marker='o', label='DP_G')
axes[1, 0].plot(x_positions, eo_egdpg_g, color='red', linestyle='--', alpha=0.3)
axes[1, 0].scatter(x_positions, eo_egdpr_g, color='orange', marker='o', label='DP_R')
axes[1, 0].plot(x_positions, eo_egdpr_g, color='orange', linestyle='--', alpha=0.3)
axes[1, 0].scatter(x_positions, eo_egeog_g, color='green', marker='s', label='EO_G')
axes[1, 0].plot(x_positions, eo_egeog_g, color='green', linestyle='--', alpha=0.3)
axes[1, 0].scatter(x_positions, eo_egeor_g, color='blue', marker='s', label='EO_R')
axes[1, 0].plot(x_positions, eo_egeor_g, color='blue', linestyle='--', alpha=0.3)

axes[1, 0].set_xticks(x_positions)
axes[1, 0].set_xticklabels(labels)
axes[1, 0].set_xlabel('Dataset Versions')
axes[1, 0].set_ylabel('Value')
axes[1, 0].set_title('Equalized Odds of Gender')
axes[1, 0].legend()

#Equalized Odds of Race (bottom right)
axes[1, 1].scatter(x_positions, eo_r, color='purple', marker='^', label='None')
axes[1, 1].plot(x_positions, eo_r, color='purple', linestyle='--', alpha=0.3)
axes[1, 1].scatter(x_positions, eo_egdpg_r, color='red', marker='o', label='DP_G')
axes[1, 1].plot(x_positions, eo_egdpg_r, color='red', linestyle='--', alpha=0.3)
axes[1, 1].scatter(x_positions, eo_egdpr_r, color='orange', marker='o', label='DP_R')
axes[1, 1].plot(x_positions, eo_egdpr_r, color='orange', linestyle='--', alpha=0.3)
axes[1, 1].scatter(x_positions, eo_egeog_r, color='green', marker='s', label='EO_G')
axes[1, 1].plot(x_positions, eo_egeog_r, color='green', linestyle='--', alpha=0.3)
axes[1, 1].scatter(x_positions, eo_egeor_r, color='blue', marker='s', label='EO_R')
axes[1, 1].plot(x_positions, eo_egeor_r, color='blue', linestyle='--', alpha=0.3)

axes[1, 1].set_xticks(x_positions)
axes[1, 1].set_xticklabels(labels)
axes[1, 1].set_xlabel('Dataset Versions')
axes[1, 1].set_title('Equalized Odds of Race')
axes[1, 1].legend()

plt.tight_layout()
plt.show()

"""Comparing Impact of Bias in the Dataset on Accuracy"""

labels = ["Less Bias", "Original", "More Bias"]

x_positions = range(len(labels))

#Accuracy
plt.scatter(x_positions, acc_1, color='purple', marker='^', label='None')
plt.plot(x_positions, acc_1, color='purple', linestyle='--', alpha=0.3)
plt.scatter(x_positions, acc_egdpg, color='red', marker='o', label='DP_G')
plt.plot(x_positions, acc_egdpg, color='red', linestyle='--', alpha=0.3)
plt.scatter(x_positions, acc_egdpr, color='orange', marker='o', label='DP_R')
plt.plot(x_positions, acc_egdpr, color='orange', linestyle='--', alpha=0.3)
plt.scatter(x_positions, acc_egeog, color='green', marker='s', label='EO_G')
plt.plot(x_positions, acc_egeog, color='green', linestyle='--', alpha=0.3)
plt.scatter(x_positions, acc_egeor, color='blue', marker='s', label='EO_R')
plt.plot(x_positions, acc_egeor, color='blue', linestyle='--', alpha=0.3)

plt.xticks(x_positions, labels)
plt.ylabel('Accuracy')
plt.xlabel('Dataset Versions')
plt.title('Model Accuracy by Amount of Bias in the Dataset')
plt.legend()
plt.show()

"""Heatmaps for each dataset
- purpose: compare all metrics/techniques in one place
- y: bias mitigation techniques
- x: fairness metrics + accuracy
- z: metrics results
"""

import seaborn as sns
from matplotlib.colors import Normalize
from matplotlib.patches import Patch

data = {  #left to right
    "DP Gender": lb_dp_gender,
    "DP Race": lb_dp_race,
    "EO Gender": lb_eo_gender,
    "EO Race": lb_eo_race,
    "Accuracy": lb_acc,
}
index = [  #top to bottom
    "None",
    "ExGrad DP Gender",
    "ExGrad DP Race",
    "ExGrad EO Gender",
    "ExGrad EO Race"
]
df = pd.DataFrame(data, index=index)

# --- Create custom color map matrix ---
fair_norm = Normalize(vmin=0, vmax=0.7)     # lower = better for fairness
acc_norm = Normalize(vmin=0.50, vmax=1.0)  # higher = better for accuracy

fair_cmap = plt.cm.Reds_r
acc_cmap = plt.cm.Greens

colors = df.copy()
for col in df.columns:
    if col == "Accuracy":
        colors[col] = df[col].apply(lambda v: acc_cmap(acc_norm(v)))
    else:
        colors[col] = df[col].apply(lambda v: fair_cmap(fair_norm(v)))

# Convert colors to list of lists for seaborn
color_matrix = colors.values.tolist()

# --- Plot heatmap with manual colors ---
fig, ax = plt.subplots(figsize=(10, 5))
sns.heatmap(
    df,
    annot=True,
    fmt=".2f",
    yticklabels=True,
    xticklabels=True,
    cbar=False,
    square=False,
    linewidths=0.5,
    linecolor='gray',
    ax=ax,
    cmap=None,
    annot_kws={"color": "white"}
)

# Override facecolors
for y in range(df.shape[0]):
    for x in range(df.shape[1]):
        ax.add_patch(plt.Rectangle((x, y), 1, 1, facecolor=color_matrix[y][x], edgecolor='gray'))

# Legend
legend_elements = [
    Patch(facecolor=fair_cmap(0.0), label="Fairness (low better)"),
    Patch(facecolor=acc_cmap(1.0), label="Accuracy (high better)")
]
ax.legend(
    handles=legend_elements,
    bbox_to_anchor=(1.05, 1),
    loc='upper left',
    borderaxespad=0.,
    title="Color meaning"
)

ax.set_title("Results for Less Bias Dataset", fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

plt.subplots_adjust(right=0.78)  # Avoid tight_layout warning
plt.show()

import seaborn as sns
from matplotlib.colors import Normalize
from matplotlib.patches import Patch

data = {  #left to right
    "DP Gender": ob_dp_gender,
    "DP Race": ob_dp_race,
    "EO Gender": ob_eo_gender,
    "EO Race": ob_eo_race,
    "Accuracy": ob_acc,
}
index = [  #top to bottom
    "None",
    "ExGrad DP Gender",
    "ExGrad DP Race",
    "ExGrad EO Gender",
    "ExGrad EO Race"
]
df = pd.DataFrame(data, index=index)

# --- Create custom color map matrix ---
fair_norm = Normalize(vmin=0, vmax=0.7)     # lower = better for fairness
acc_norm = Normalize(vmin=0.50, vmax=1.0)  # higher = better for accuracy

fair_cmap = plt.cm.Reds_r
acc_cmap = plt.cm.Greens

colors = df.copy()
for col in df.columns:
    if col == "Accuracy":
        colors[col] = df[col].apply(lambda v: acc_cmap(acc_norm(v)))
    else:
        colors[col] = df[col].apply(lambda v: fair_cmap(fair_norm(v)))

# Convert colors to list of lists for seaborn
color_matrix = colors.values.tolist()

# --- Plot heatmap with manual colors ---
fig, ax = plt.subplots(figsize=(10, 5))
sns.heatmap(
    df,
    annot=True,
    fmt=".2f",
    yticklabels=True,
    xticklabels=True,
    cbar=False,
    square=False,
    linewidths=0.5,
    linecolor='gray',
    ax=ax,
    cmap=None,
    annot_kws={"color": "white"}
)

# Override facecolors
for y in range(df.shape[0]):
    for x in range(df.shape[1]):
        ax.add_patch(plt.Rectangle((x, y), 1, 1, facecolor=color_matrix[y][x], edgecolor='gray'))

# Legend
legend_elements = [
    Patch(facecolor=fair_cmap(0.0), label="Fairness (low better)"),
    Patch(facecolor=acc_cmap(1.0), label="Accuracy (high better)")
]
ax.legend(
    handles=legend_elements,
    bbox_to_anchor=(1.05, 1),
    loc='upper left',
    borderaxespad=0.,
    title="Color meaning"
)

ax.set_title("Results for Original Bias Dataset", fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

plt.subplots_adjust(right=0.78)  # Avoid tight_layout warning
plt.show()

import seaborn as sns
from matplotlib.colors import Normalize
from matplotlib.patches import Patch

data = {  #left to right
    "DP Gender": mb_dp_gender,
    "DP Race": mb_dp_race,
    "EO Gender": mb_eo_gender,
    "EO Race": mb_eo_race,
    "Accuracy": mb_acc,
}
index = [  #top to bottom
    "None",
    "ExGrad DP Gender",
    "ExGrad DP Race",
    "ExGrad EO Gender",
    "ExGrad EO Race"
]
df = pd.DataFrame(data, index=index)

# --- Create custom color map matrix ---
fair_norm = Normalize(vmin=0, vmax=0.7)     # lower = better for fairness
acc_norm = Normalize(vmin=0.50, vmax=1.0)  # higher = better for accuracy

fair_cmap = plt.cm.Reds_r
acc_cmap = plt.cm.Greens

colors = df.copy()
for col in df.columns:
    if col == "Accuracy":
        colors[col] = df[col].apply(lambda v: acc_cmap(acc_norm(v)))
    else:
        colors[col] = df[col].apply(lambda v: fair_cmap(fair_norm(v)))

# Convert colors to list of lists for seaborn
color_matrix = colors.values.tolist()

# --- Plot heatmap with manual colors ---
fig, ax = plt.subplots(figsize=(10, 5))
sns.heatmap(
    df,
    annot=True,
    fmt=".2f",
    yticklabels=True,
    xticklabels=True,
    cbar=False,
    square=False,
    linewidths=0.5,
    linecolor='gray',
    ax=ax,
    cmap=None,
)

# Override facecolors
for y in range(df.shape[0]):
    for x in range(df.shape[1]):
        ax.add_patch(plt.Rectangle((x, y), 1, 1, facecolor=color_matrix[y][x], edgecolor='gray'))

# Legend
legend_elements = [
    Patch(facecolor=fair_cmap(0.0), label="Fairness (low better)"),
    Patch(facecolor=acc_cmap(1.0), label="Accuracy (high better)")
]
ax.legend(
    handles=legend_elements,
    bbox_to_anchor=(1.05, 1),
    loc='upper left',
    borderaxespad=0.,
    title="Color meaning"
)

ax.set_title("Results for More Bias Dataset", fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

plt.subplots_adjust(right=0.78)  # Avoid tight_layout warning
plt.show()